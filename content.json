{"meta":{"title":"zzlzyl","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"Spark local & stand-alone配置-中文文档","slug":"Spark local& stand-alone配置","date":"2022-05-17T12:18:47.360Z","updated":"2022-05-17T03:15:47.676Z","comments":true,"path":"2022/05/17/Spark local& stand-alone配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群) Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试 Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在linux系统之上 hadoop YARN 模式:Spark中的各个角色运行在YARN的容器内部,并组成Spark集群环境,运行在yarn容器内 Spark中的各个角色运行在Kubernetes的容器内部,并组成Spark集群环境 本文着重描述 Spark local 模式和 Spark alone 模式 Spark local模式Spark local 模式是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时的环境Spark由四类角色组成整个Spark的运行环境: Master角色，管理整个集群的资源 Worker角色，管理单个服务器的资源 Driver角色，管理单个Spark任务在运行的时候的工作 Executor角色，单个任务运行的时候的工作者 安装注意：以下操作需要完成 Spark 基础环境配置。具体配置移步到Spark基础环境配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Spark基础环境配置","slug":"Spark基础环境配置","date":"2022-05-17T12:18:46.617Z","updated":"2022-05-17T12:18:18.705Z","comments":true,"path":"2022/05/17/Spark基础环境配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"","text":"本文主 本文主要使用Hexo与Github进行个人blog的搭建Hexo官网：HexoGithub官网：Github 环境介绍本地环境为: Window10系统、Linux虚拟机 开始搭建1.基础环境在开始配置前，需要检查虚拟机主机名、hosts映射、关闭防火墙、免密登录、同步时间等操作 （1）编辑主机名（三台机器）123456789#查看系统主机名(三台主机)$ cat /etc/hostname#在三台主机上更改主机名#在 master 主节点$ echo &quot;master&quot; &gt;/etc/hostname #在 slave1 节点 $ echo &quot;slave1&quot; &gt;/etc/hostname #在 slave2 节点$ echo &quot;slave2&quot; &gt;/etc/hostname （2）hosts映射12345678910111213#查看系统映射$ cat /etc/hosts#编辑 /etc/hosts 文件$ vim /etc/hosts#内容修改为 （三台主机内容一致）127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 master 192.168.88.136 slave1 192.168.88.137 slave2 （3）关闭防火墙1234#关闭防火墙$ systemctl stop firewalld.service#禁止防火墙开启自启$ systemctl disable firewalld.service （4）免密登录123456#master 生成公钥私钥，四个回车即可$ ssh-keygen#master 配置免密登录到master slave1 slave2三台主机$ ssh-copy-id master $ ssh-copy-id slave1 $ ssh-copy-id slave2 （5）时间同步123456#安装 ntp$ yum install ntp -y #设置 ntp 开机自启动$ systemctl enable ntpd &amp;&amp; systemctl start ntpd#三台主机分别运行以下命令$ ntpdate ntp4.aliyun.com 2.JDK安装（1）下载安装包本文使用的 JDK 是1.8版本jdk1.8安装包下载注意：下载的是后缀为 .tar.gz 的包 （2）在主机 master 上安装 JDK12345678910111213141516#编译环境软件安装目录$ mkdir -p /export/server#上传本地下载好的jdk-8u241-linux-x64.tar.gz到/export/server/目录下 并解压文件$ tar -zxvf jdk-8u241-linux-x64.tar.gz#配置环境变量$ vim /etc/profile#在文件内添加如下内容# jdk 环境变量 export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.ja#重新加载环境变量文件$ source /etc/profile#查看 java 版本号$ java -version#出现 java version &quot;1.8.0_241&quot; 表示安装成功 （3）分发12345678910#master 节点将 java 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ $ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/#配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s jdk1.8.0_241/ jdk#重新加载环境变量文件$ source /etc/profile 3.Hadoop安装（1）下载安装包本文使用的 hadoop 是3.3.0版本hadoop3.3.0安装包下载注意：下载的是后缀为 .tar.gz的包 （2）在主机 master 上安装 hadoop123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#上传本地下载好的 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 /export/server 并解压文件$ tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz#修改配置文件,进入到 hadoop 目录下$ cd /export/server/hadoop-3.3.0/etc/hadoop#编辑 hadoop-env.sh 文件$ vim hadoop-env.sh#文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root#修改 core-site.xml 文件 $ vim core-site.xml #添加如下内容&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;hdfs-site.xmlmapred-site.xml&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;#修改 mapred-site.xml 文件$ vim mapred-site.xml#添加如下内容&lt;!-- 设置MR程序默认运行模式： yarn 集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;yarn-site.xmlworkers#修改 hdfs-site.xml 文件$ vim hdfs-site.xml#添加如下内容&lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:9868&lt;/value&gt; &lt;/property&gt;#修改 yarn-site.xml 文件$ vim yarn-site.xml#添加如下内容&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;#修改 workers 文件$ vim workers#将 workers 里的 localhost 删除，添加如下内容master slave1 slave2 （3）分发12345678910111213141516171819202122232425#master 节点将 hadoop 传输到 slave1 和 slave2$ cd /export/server$ scp -r hadoop-3.3.0 root@node2:$PWD$ scp -r hadoop-3.3.0 root@node3:$PWD#将 hadoop 添加到环境变量vim /etc/profile#在文件内添加如下内容# hadoop 环境变量 export HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s hadoop-3.3.0/ hadoop#重新加载环境变量文件$ source /etc/profile#在 master 主节点进行 Hadoop 集群启动 格式化 namenode（只有首次启动需要格式化）$ hdfs namenode -format#等待初始化完成后，使用脚本一键起动$ start-all.sh #起动后，输入jps查看进程号$ jps#进程查看完毕后可进入到 WEB 界面#HDFS集群的界面网站是:http://master:9870/#YARN集群的界面网站是:http://master:9870/ 4.安装zookeeper（1）下载安装包本文使用的 zookeeper 是3.7.0版本zookeeper3.7.0安装包下载注意：下载的是后缀为 .tar.gz 的包,安装包需要3.7版本网上，否者后续spark配置会出现问题 （2）在主机 master 上安装 zookeeper12345678910111213141516171819202122232425262728293031#上传本地下载好的 apache-zookeeper-3.7.0-bin.tar.gz 到 /export/server 并解压文件$ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz#修改配置文件,进入到 /export/server 目录下$ cd /export/server/#在 /export/server 目录下创建 zookeeper 软连接$ ln -s apache-zookeeper-3.7.0-bin/ zookeeper#进入到 zookeeper 目录下$ cd zookeeper#进入到 zookeeper 下的 conf 文件内$ cd /export/server/zookeeper/conf/ #将 zoo_sample.cfg 文件复制为新文件 zoo.cfg$ cp zoo_sample.cfg zoo.cfg#在 zoo.cfg 文件内添加如下内容#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888#进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件,将 1 写入进去$ cd /export/server/zookeeper/zkdata$ mkdir myid$ echo &#x27;1&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字1即为成功 （3）分发123456789101112131415161718192021222324252627282930313233#master 节点将 zookeeper 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/zookeeper/ slave1:$PWD$ scp -r /export/server/zookeeper/ slave2:$PWD#推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和 3#在 slave1 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;2&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字2即为成功#在 slave2 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;3&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字3即为成功#将 zookeeper 添加到环境变量vim /etc/profile#在文件内添加如下内容# zookeeper 环境变量 export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#重新加载环境变量文件$ source /etc/profile#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本$ cd /export/server/zookeeper/bin$ zkServer.sh start#查看 zookeeper 的状态$ zkServer.sh status#也可以通过jps查看zookeeper的进程$ jps 以上,就是Spark基础环境的配置,接下来会带来 Spark local&amp; stand-alone配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]}
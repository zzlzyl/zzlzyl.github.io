{"meta":{"title":"Spark使用","subtitle":"让你感受专业的配置","description":"基于Hexo来展示Spark配置","author":"zzl","url":"http://example.com","root":"/"},"pages":[{"title":"Categories","date":"2022-05-17T01:00:30.000Z","updated":"2022-05-17T01:03:56.938Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags-zzl","date":"2022-05-19T07:08:21.359Z","updated":"2022-05-19T07:08:21.359Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"About","date":"2022-05-17T01:04:07.000Z","updated":"2022-05-17T01:05:07.824Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"404","date":"2022-05-17T01:05:30.000Z","updated":"2022-05-17T01:07:08.856Z","comments":true,"path":"404.html","permalink":"http://example.com/404.html","excerpt":"","text":""}],"posts":[{"title":"Spark HA & Yarn配置","slug":"Spark HA & Yarn配置","date":"2022-05-19T08:19:25.354Z","updated":"2022-05-19T10:44:31.962Z","comments":true,"path":"2022/05/19/Spark HA & Yarn配置/","link":"","permalink":"http://example.com/2022/05/19/Spark%20HA%20&%20Yarn%E9%85%8D%E7%BD%AE/","excerpt":"","text":"本文着重描述 Spark HA 模式和 Spark on Yarn 模式注意：以下操作是基于Spark local &amp; stand-alone配置,若未操作,请移步到[Spark local &amp; stand-alone配置](..&#x2F;..&#x2F;17&#x2F;Spark local &amp; stand-alone配置)进行配置 1.Spark HA 模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题；aster故障后，集群就不可用。在HA模式下当Active的Master出现故障时,另外的一个Standby Master会被选举出来 （1）更改 Spark 配置文件内容123#在虚拟机 master 上进入到/export/server/spark/conf/目录下$ cd /export/server/spark/conf/#向conf文件下的 spark-env.sh 文件添加内容 **在进行添加内容之前,要删除或者注释掉 export SPARK_MASTER_HOST&#x3D;master **代码位于文件83行,要显示行数可通过:set nu查看 123456$ vim spark-env.sh#在文件末添加如下内容SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER - Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha&quot; # spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 # 指定Zookeeper的连接地址 # 指定在Zookeeper中注册临时节点的路径 （2）分发1234#将更改的文件到slave1、slave2上$ cd /export/server/spark/conf/$ scp spark-env.sh slave1:/export/server/spark/conf/$ scp spark-env.sh slave2:/export/server/spark/conf/ （3）开启三台主机启动Zookeeper 1234567#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本$ cd /export/server/zookeeper/bin$ zkServer.sh start#查看 zookeeper 的状态$ zkServer.sh status#也可以通过jps查看zookeeper的进程$ jps master启动hadoop 12345#在 master 主节点启动hadoop#使用脚本一键起动$ start-all.sh #起动后，输入jps查看进程号$ jps master主节点启动master和worker进程 1234#在 master 主节点进入到 /export/server/spark/sbin/ 目录下$ cd /export/server/spark/sbin/ #开启master和worker进程$ sh start-all.sh 访问 WebUI 界面http://master:8080/出现访问不成功的时候可能是端口被占用，将8080换为8081、8082….进行尝试 访问WebUI界面，查看slave1和slave2的状态，可以看到slave1的状态为ALIVE，slave2状态为STANDBY 此时 kill 掉 master 上的 master 的 16551 进程号，通过jps查看端口是否删除 123#kill 进程16551$ kill -9 16551$ jps 注意:删除的进程号,需要是主机master通过jps查看后的Master进程号刷新node2WebUI界面，发现其状态由STANDBY变为ALIVE此时master不能在访问,需要更改端口号才能访问,逐次增加1进行尝试 2.Spark on Yarn 模式在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端 Spark在YARN上的角色 Master角色由YARN的ResourceManager担任 Worker角色由YARN的NodeManager担任. Driver角色运行在YARN容器内或提交任务的客户端进程中 Executor运行在YARN提供的容器内 （1）更改 Spark 配置文件内容12345678#在虚拟机 master 上进入到/export/server/spark/conf/目录下$ cd /export/server/spark/conf/#向conf文件下的 spark-env.sh 文件添加内容$ vim spark-env.sh#在文件末添加如下内容## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop （2）运行在YARN上运行spark 123#可以直接运行以下命令$ /export/server/spark/bin/pyspark --master yarn#出现Spark表示,表明可以在YARN集群上运行spark client模式测试 1234#进入到 /export/server/spark/$ cd /export/server/spark/#进行 client 测试$ /bin/spark-submit --master yarn --deploy-mode client -- driver-memory 512m --executor-memory 512m --num-executors 1 --total- executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 cluster模式测试 1234#进入到 /export/server/spark/$ cd /export/server/spark/#进行 cluster 测试$ /bin/spark-submit --master yarn --deploy-mode cluster -- driver-memory 512m --executor-memory 512m --num-executors 1 --total- executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 若要查看它的具体运行情况，需要开启hadoop的历史服务 1234#进入到 /export/server/hadoop/sbin$ cd /export/server/hadoop/sbin#启动 hadoop 历史服务器$ ./mr-jobhistory-daemon.sh start historyserver 启动历史服务器后,访问WebUI界面，查看client模式下的运行状况 启动历史服务器后,访问WebUI界面，查看cluster模式下的运行状况 以上,就是 Spark HA &amp; Yarn配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Spark local & stand-alone配置","slug":"Spark local& stand-alone配置","date":"2022-05-17T12:18:47.360Z","updated":"2022-05-19T10:40:48.290Z","comments":true,"path":"2022/05/17/Spark local& stand-alone配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群) Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试 Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在linux系统之上 hadoop YARN 模式:Spark中的各个角色运行在YARN的容器内部,并组成Spark集群环境,运行在yarn容器内 Spark中的各个角色运行在Kubernetes的容器内部,并组成Spark集群环境 本文着重描述 Spark local 模式和 Spark alone 模式1.Spark local 模式Spark local 模式是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时的环境Spark由四类角色组成整个Spark的运行环境: Master角色，管理整个集群的资源 Worker角色，管理单个服务器的资源 Driver角色，管理单个Spark任务在运行的时候的工作 Executor角色，单个任务运行的时候的工作者 注意：以下操作需要完成 Spark 基础环境配置。具体配置移步到Spark基础环境配置 （1）Anaconda安装Anaconda安装包下载，不建议去官网下载，官网下载太慢了，推荐下载地址Anaconda清华镜像站注意：下载的是 Anaconda3-2021.05-Linux-x86_64.sh 后缀为 .sh 的安装包 1234#上传本地下载好的 Anaconda 安装包上到 /export/server/ 目录下进行安装$ cd /export/server#执行文件$ sh Anaconda3-2021.05-Linux-x86_64.sh 1)在遇到 Do you accept the license terms? [yes|no]时，选择yes2)在上述命令回车后，会让你选择你想要安装的路径，统一安装在&#x2F;export&#x2F;server&#x2F;anaconda3下3)等待执行完毕后，在新的[yes|no]选择界面选择yes；随后exit退出重新登录即可看到base，代表着安装完成 1234#创建虚拟环境pyspark，基于Python 3.8$ conda create -n pyspark python=3.8#切换到虚拟环境内$ conda activate pyspark 4)看到(pyspark)表示成功 12#在虚拟环境内安装包$ pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple （2）Spark安装本文使用的 spark 是3.2.0版本spark 3.2.0 安装包下载注意：下载要注意hadoop的版本，同时选择后缀为 .tar.gz的安装包本文使用的安装包是spark-3.2.0-bin-hadoop3.2.tgz 1234567891011121314#把本地下载好的spark-3.2.0-bin-hadoop3.2.tgz安装包上传到 /export/server 并解压$ cd /export/server/$ tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/#建立软连接$ ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark#编辑环境变量$ vim /etc/profile#在文件末添加以下内容#SPARK_HOME export SPARK_HOME=/export/server/spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 1234567#编辑.bashrc文件，添加java和pyspark的Home值$ vim .bashrc#添加以下内容#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 1234567#重新加载环境变量文件$ source /etc/profile$ source ~/.bashrc#进入 /export/server/anaconda3/envs/pyspark/bin/ 文件夹$ cd /export/server/anaconda3/envs/pyspark/bin/#开启 Spark$ ./pyspark 看到 pyspark 表示成功 123#测试运行基于python的spark解释器环境，在下方运行以下代码$ sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()#结果出来后,查看WebUI界面:http://master:4040/ 2.Spark alone 模式tand-alone集群模式中，Spark的各个角色以独立进程的形式存在，并组成Spark集群环境StandAlone集群在进程上主要有三类: 主节点Master进程：Master角色，管理整个集群资源，并托管各个任务的Driver 从节点Workers：Worker角色，管理每个机器的资源，分配对应资源来运行Executor（Task） 历史服务器HistoryServer：在Spark Application运行完成以后，保存事件日志数据至HDFS （1）Spark alone 模式下的新配置前提:三台虚拟机全部安装 Anaconda参考Spark（local）模式下的Anaconda的安装文档，在slave1、slave2完成对Anaconda的安装 12#master 节点节点进入 /export/server/spark/conf 修改以下配置文件$ cd /export/server/spark/conf 12345678#将文件 workers.template 改名为 workers，并配置文件内容$ mv workers.template workers#修改 workers 文件$ vim workers#将 workers 里的 localhost 删除，添加如下内容master slave1 slave2 12345678910111213141516171819202122232425262728#将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容$ mv spark-env.sh.template spark-env.sh#修改 spark-env.sh 文件$ vim spark-env.sh#文件最后添加以下内容## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=master # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=&quot;- Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - Dspark.history.fs.cleaner.enabled=true&quot; 1234567#开启hadoop服务$ start-all.sh#在HDFS上创建程序运行历史记录存放文件夹$ hadoop fs -mkdir /sparklog#已存在会显示File exists,属于正常状况#给sparklog添加权限$ hadoop fs -chmod 777 /sparklog 12345678910111213#重新进入到 /export/server/spark/conf/ 目录下$ cd /export/server/spark/conf/#将spark-defaults.conf.template 文件改为spark-defaults.conf$ mv spark-defaults.conf.template spark-defaults.conf#编辑 spark-defaults.conf$ vim spark-defaults.conf#文件最后添加以下内容# 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://master:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true 12#将log4j.properties.template文件改为log4j.properties$ mv log4j.properties.template log4j.properties 配置 log4j.properties 文件将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为log4j.rootCategory&#x3D;WARN, console （2）分发1234#master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上$ cd /export/server$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave1:$PWD$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave2:$PWD 在slave1 和 slave2 上做软连接 1234#在 slave1 节点上$ ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark#重新加载环境变量$ source /etc/profile 1234#在 slave2 节点上$ ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark#重新加载环境变量$ source /etc/profile 完成上述操作后，启动历史服务器 12345#在 master 主机上进入 /export/server/spark/sbin 文件目录下$ cd /export/server/spark/sbin#启动历史服务器$ ./start-history-server.sh#访问 WebUI 界面:http://master:18080/ 123456#启动Spark的Master和Worker进程$ sh start-all.sh#通过jps查看进程是否开启master和worker进程$ jps 在查看到 Master 和 Worker 进程后表明成功启动#访问 WebUI界面:http://master:8080/ 以上,就是 Spark local&amp; stand-alone配置,接下来会带来 Spark HA &amp; Yarn配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Spark基础环境配置","slug":"Spark基础环境配置","date":"2022-05-17T12:18:46.617Z","updated":"2022-05-19T10:35:50.237Z","comments":true,"path":"2022/05/17/Spark基础环境配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"","text":"本文主要使用Hexo与Github进行个人blog的搭建Hexo官网：HexoGithub官网：Github 环境介绍本地环境为: Window10系统、Linux虚拟机 开始搭建1.基础环境在开始配置前，需要检查虚拟机主机名、hosts映射、关闭防火墙、免密登录、同步时间等操作 （1）编辑主机名（三台机器）123456789#查看系统主机名(三台主机)$ cat /etc/hostname#在三台主机上更改主机名#在 master 主节点$ echo &quot;master&quot; &gt;/etc/hostname #在 slave1 节点 $ echo &quot;slave1&quot; &gt;/etc/hostname #在 slave2 节点$ echo &quot;slave2&quot; &gt;/etc/hostname （2）hosts映射12345678910111213#查看系统映射$ cat /etc/hosts#编辑 /etc/hosts 文件$ vim /etc/hosts#内容修改为 （三台主机内容一致）127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 master 192.168.88.136 slave1 192.168.88.137 slave2 （3）关闭防火墙1234#关闭防火墙$ systemctl stop firewalld.service#禁止防火墙开启自启$ systemctl disable firewalld.service （4）免密登录123456#master 生成公钥私钥，四个回车即可$ ssh-keygen#master 配置免密登录到master slave1 slave2三台主机$ ssh-copy-id master $ ssh-copy-id slave1 $ ssh-copy-id slave2 （5）时间同步123456#安装 ntp$ yum install ntp -y #设置 ntp 开机自启动$ systemctl enable ntpd &amp;&amp; systemctl start ntpd#三台主机分别运行以下命令$ ntpdate ntp4.aliyun.com 2.JDK安装（1）下载安装包本文使用的 JDK 是1.8版本jdk1.8安装包下载注意：下载的是后缀为 .tar.gz 的包 （2）在主机 master 上安装 JDK1234567891011#编译环境软件安装目录$ mkdir -p /export/server#上传本地下载好的jdk-8u241-linux-x64.tar.gz上传到/export/server/目录下 并解压文件$ tar -zxvf jdk-8u241-linux-x64.tar.gz#配置环境变量$ vim /etc/profile#在文件内添加如下内容# jdk 环境变量 export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.ja 12345#重新加载环境变量文件$ source /etc/profile#查看 java 版本号$ java -version#出现 java version &quot;1.8.0_241&quot; 表示安装成功 （3）分发12345678910#master 节点将 java 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ $ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/#配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s jdk1.8.0_241/ jdk#重新加载环境变量文件$ source /etc/profile 3.Hadoop安装（1）下载安装包本文使用的 hadoop 是3.3.0版本hadoop3.3.0安装包下载注意：下载的是后缀为 .tar.gz的包 （2）在主机 master 上安装 hadoop1234567891011121314#上传本地下载好的 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件$ tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz#修改配置文件,进入到 hadoop 目录下$ cd /export/server/hadoop-3.3.0/etc/hadoop#编辑 hadoop-env.sh 文件$ vim hadoop-env.sh#文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root 1234567891011121314151617181920212223242526272829303132333435#修改 core-site.xml 文件 $ vim core-site.xml #添加如下内容&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;hdfs-site.xmlmapred-site.xml&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; 123456789101112131415161718192021222324252627282930#修改 mapred-site.xml 文件$ vim mapred-site.xml#添加如下内容&lt;!-- 设置MR程序默认运行模式： yarn 集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;yarn-site.xmlworkers 12345678#修改 hdfs-site.xml 文件$ vim hdfs-site.xml#添加如下内容&lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:9868&lt;/value&gt; &lt;/property&gt; 12345678910111213141516171819202122232425262728293031323334353637#修改 yarn-site.xml 文件$ vim yarn-site.xml#添加如下内容&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 123456#修改 workers 文件$ vim workers#将 workers 里的 localhost 删除，添加如下内容master slave1 slave2 （3）分发1234#master 节点将 hadoop 传输到 slave1 和 slave2$ cd /export/server$ scp -r hadoop-3.3.0 root@node2:$PWD$ scp -r hadoop-3.3.0 root@node3:$PWD 123456#将 hadoop 添加到环境变量vim /etc/profile#在文件内添加如下内容# hadoop 环境变量 export HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 123456789101112131415#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s hadoop-3.3.0/ hadoop#重新加载环境变量文件$ source /etc/profile#在 master 主节点进行 Hadoop 集群启动 格式化 namenode（只有首次启动需要格式化）$ hdfs namenode -format#等待初始化完成后，使用脚本一键起动$ start-all.sh #起动后，输入jps查看进程号$ jps#进程查看完毕后可进入到 WEB 界面#HDFS集群的界面网站是:http://master:9870/#YARN集群的界面网站是:http://master:9870/ 4.安装zookeeper（1）下载安装包本文使用的 zookeeper 是3.7.0版本zookeeper3.7.0安装包下载注意：下载的是后缀为 .tar.gz 的包,安装包需要3.7版本网上，否者后续spark配置会出现问题 （2）在主机 master 上安装 zookeeper12345678#上传本地下载好的 apache-zookeeper-3.7.0-bin.tar.gz 上传到 /export/server 并解压文件$ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz#修改配置文件,进入到 /export/server 目录下$ cd /export/server/#在 /export/server 目录下创建 zookeeper 软连接$ ln -s apache-zookeeper-3.7.0-bin/ zookeeper#进入到 zookeeper 目录下$ cd zookeeper 12345678910111213141516#进入到 zookeeper 下的 conf 文件内$ cd /export/server/zookeeper/conf/ #将 zoo_sample.cfg 文件复制为新文件 zoo.cfg$ cp zoo_sample.cfg zoo.cfg#在 zoo.cfg 文件内添加如下内容#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888 1234567#进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件,将 1 写入进去$ cd /export/server/zookeeper/zkdata$ mkdir myid$ echo &#x27;1&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字1即为成功 （3）分发12345#master 节点将 zookeeper 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/zookeeper/ slave1:$PWD$ scp -r /export/server/zookeeper/ slave2:$PWD#推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和 3 123456#在 slave1 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;2&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字2即为成功 123456#在 slave2 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;3&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字3即为成功 123456#将 zookeeper 添加到环境变量vim /etc/profile#在文件内添加如下内容# zookeeper 环境变量 export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin 12345678910#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#重新加载环境变量文件$ source /etc/profile#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本$ cd /export/server/zookeeper/bin$ zkServer.sh start#查看 zookeeper 的状态$ zkServer.sh status#也可以通过jps查看zookeeper的进程$ jps 以上,就是Spark基础环境的配置,接下来会带来 Spark local&amp; stand-alone配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]}
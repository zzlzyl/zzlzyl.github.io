{"meta":{"title":"Spark使用","subtitle":"让你感受专业的配置","description":"基于Hexo来展示Spark配置","author":"zzl","url":"http://example.com","root":"/"},"pages":[{"title":"404","date":"2022-05-17T01:05:30.000Z","updated":"2022-05-17T01:07:08.856Z","comments":true,"path":"404.html","permalink":"http://example.com/404.html","excerpt":"","text":""},{"title":"About","date":"2022-05-17T01:04:07.000Z","updated":"2022-05-17T01:05:07.824Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Tags-zzl","date":"2022-05-19T07:08:21.359Z","updated":"2022-05-19T07:08:21.359Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-05-17T01:00:30.000Z","updated":"2022-05-17T01:03:56.938Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka命令行操作 ","slug":"Kafka命令行操作","date":"2022-06-15T11:58:03.012Z","updated":"2022-06-15T13:11:31.908Z","comments":true,"path":"2022/06/15/Kafka命令行操作/","link":"","permalink":"http://example.com/2022/06/15/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/","excerpt":"","text":"注意：以下操作需要完成 Kafka 基础环境配置。具体配置移步到Kafka基础环境配置 1.Kafka安装本文使用的 Kafka 是2.11-2.0.0版本Kafka 3.2.0 安装包下载注意：下载要注意 Kafka 的版本，同时选择后缀为 .tar.gz的安装包**本文使用的安装包是 kafka_2.11-2.0.0.tgz ** 1234567#把本地下载好的 kafka_2.11-2.0.0.tgz安装包上传到 /export/server 并解压$ cd /export/server/$ tar -zxvf kafka_2.11-2.0.0.tgz -C /export/server/#建立软连接$ ln -s /export/server/kafka_2.11-2.0.0.tgz /export/server/kafka#master 节点节点进入 /export/server/kafka/config 修改以下配置文件$ cd /export/server/kafka/config 编辑文件 server.properties12345678910111213141516171819$ vim server.properties#21 行内容 broker.id=0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复#（注：此处为master节点，不用修改）$broker.id=0#31 行内容 #listeners=PLAINTEXT://:9092 取消注释，内容改为$ listeners=PLAINTEXT://master:9092#59 行内容 log.dirs=/tmp/kafka-logs 为默认日志文件存储的位置，改为$log.dirs=/export/server/data/kafka-log#63 行内容为 num.partitions=1 是默认分区数$ num.partitions=1#121 行内容 zookeeper.connect=localhost:2181 修改为$zookeeper.connect=master:2181,slave1:2181,slave2:2181#126 行内容 group.initial.rebalance.delay.ms=0 修改为$ group.initial.rebalance.delay.ms=3000 2.分发1234#master 节点分发 kafka 安装文件夹 到 slave1 和 slave2 上$ cd /export/server$ scp -r /export/server/kafka_2.11-2.0.0.tgz/ slave1:$PWD$ scp -r /export/server/kafka_2.11-2.0.0.tgz/ slave2:$PWD 123456#配置 kafka 环境变量，master、slave1、slave2都需要进行操作#vim /etc/profile#文件最后添加以下内容# kafka 环境变量 $ export KAFKA_HOME=/export/server/kafka $ export PATH=$PATH:$KAFKA_HOME/bin 12#三台机器操作完成重新加载环境变量$ source /etc/profile 12345#在 slave1 节点上$ cd /export/server$ ln -s /export/server/kafka_2.11-2.0.0/ kafka#进入 /export/server/kafka/config 修改以下配置文件$ cd /export/server/kafka/config 12345$ vim server.properties#将文件 server.properties 的第 21 行的 broker.id=0 修改为 $ broker.id=1 #将文件 server.properties 的第 31 行的 listeners=PLAINTEXT://master:9092 修改为$ listeners=PLAINTEXT://slave1:9092 12345#在 slave2 节点上$ cd /export/server$ ln -s /export/server/kafka_2.11-2.0.0/ kafka#进入 /export/server/kafka/config 修改以下配置文件$ cd /export/server/kafka/config 12345$ vim server.properties#将文件 server.properties 的第 21 行的 broker.id=0 修改为 $ broker.id=2 #将文件 server.properties 的第 31 行的 listeners=PLAINTEXT://master:9092 修改为$ listeners=PLAINTEXT://slave2:9092 以上操作完成回到 master 节点 启动 kafka注意：启动 kafka 需要启动 zookeeper 123#启动 kafka,三台机器同时执行$ kafka-server-start.sh -daemon /export/server/kafka/config/server.properties#启动完成后通过jps查看其状态 设置脚本便于启动三台机器1234567891011121314151617181920212223242526272829#进入到bin目录下$cd /root/bin# 创建并编辑名为kafka-all.sh的脚本$ vim kafka-all.sh#在文件内添加如下内容#!/bin/bashif [ $# -eq 0 ] ;thenecho &quot;please input param:start stop&quot;elseif [ $1 = start ] ;then echo &quot;$&#123;1&#125;ing master&quot;ssh master &quot;source /etc/profile;kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;for i in &#123;1..2&#125;do echo &quot;$&#123;1&#125;ing slave$&#123;i&#125;&quot; ssh slave$&#123;i&#125; &quot;source /etc/profile;kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;donefiif [ $1 = stop ];thenecho &quot;$&#123;1&#125;ping master &quot;ssh master &quot;source /etc/profile;kafka-server-stop.sh&quot;for i in &#123;1..2&#125;do echo &quot;$&#123;1&#125;ping slave$&#123;i&#125;&quot; ssh slave$&#123;i&#125; &quot;source /etc/profile;kafka-server-stop.sh&quot;donefifi 3.Kafka命令行操作（1）创建topic1$ kafka-configs.sh --create --topic tpc_1 --partitions 2 --replication-factor2 --zookeeper node1:2181 （2）删除topic1$ kafka-topics.sh --delete --topic tpc_1 --zookeeper node1:2181 （3）查看topic1234#查看当前系统中的所有topic$ kafka-topics.sh --zookeeper node1:2181-list#查看topic详细信息$ kafka-configs.sh --create --topic tpc_1 --zookeeper node1:2181--replication-factor0:1 （4）增加分区数1kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181 （5）动态配置topic参数12345#通过管理命令，可以为已创建的topic增加，修改，删除 topic level 参数#添加，修改配置参数$ kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip#删除配置参数$ kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Kafka API使用方法","slug":"Kafka API使用方法","date":"2022-06-15T07:46:15.227Z","updated":"2022-06-15T15:10:17.681Z","comments":true,"path":"2022/06/15/Kafka API使用方法/","link":"","permalink":"http://example.com/2022/06/15/Kafka%20API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","excerpt":"","text":"1.生产者API 一个正常的生产逻辑需要具备以下几个步骤: 配置生产者客户端参数及创建相应的生产者实例 构建待发送的消息 发送消息 关闭生产者实例 acks 模式：取值0,1，-1（all）； 0：Producer往集群发送数据不需要等到集群的返回，不确保信息发送成功，安全性最低但是效率最高 1：Producer往集群发送数据只要Leader成功写入消息就能发送下一条，只确保Leader接收成功 -1（all）：确保Leader发送成功，所有的副本也发送成功，过程虽然缓慢但是安全性最高； 生产者API采用默认分区方式将消息散列的发送到各个分区当中； 123456789101112131415161718192021222324252627$ Properties props = new Properties()配置生产者客户端参数$ props.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node2:9092, node3:9092&quot;)设置kafka集群的地址$props.put(“retries”, 3)失败重试次数，失败后会自动重试（可恢复/不可恢复）→(有可能会造成数据的乱序)$props.put(“batch.size”, 10)数据发送的批次大小，提高效率/吞吐量大会数据延迟$props.put(&quot;linger.ms&quot;, 10000)消息在缓冲区保留的时间,超过设置的值就会被提交到服务端$props.put(&quot;max.request.size&quot;,10)数据发送请求的最大缓存数$props.put(“buffer.memory”, 10240)整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端//buffer.memory要大于batch.size,否则会报申请内存不足的错误降低阻塞的可能性$props.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common. serialization.StringSerializer&quot;)key-value序列化器$props.put(“value.serializer”, “org.apache.kafka.common. serialization.StringSerializer”)字符串最好 Kafka 生产者客户端 KatkaProducer 中的三个必要参数bootstrap.servers 、key.serializer 、value.serializer； 生产者api参数发送方式（发后即忘）: 发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。 在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失; 这种发送方式的性能最高,可靠性最差。 生产者api参数发送方式（同步发送）: producer.send(rcd).get( ); &#x2F;&#x2F;一旦调用get方法，就会阻塞 Future future &#x3D; Callable.run( )&#x2F;&#x2F;有返回值，future.get（） runnable.run（）&#x2F;&#x2F;无返回值 生产者api参数发送方式（异步发送）； 回调函数会在producer收到 ack 时调用,为异步调用, 该方法有两个参数,分别是RecordMetadata和Exception,如果Exception为null,说明消息发送成功,如果 Exception不为null,说明消息发送失败; 注意：消息发送失败会自动重试,不需要我们在回调函数中手动重试。 在IDEA中创建名为rgzn_kafka_zzl_0330的项目，在出现的pom.xml文件内添加需要的配置，重启rgzn_kafka_zzl_0330项目 ProducerCallbackDemo类 生产者原理: 12345678910111213141516（1）一个生产者客户端由两个线程协调运行,这两个线程分别为主线程和 Sender 线程；（2）在主线程中由kafkaProducer创建消息,然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(RecordAccumulator, 也称为消息收集器)中；（3）Sender线程负责从RecordAccumulator 获取消息并将其发送到Kafka中；（4）RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能；（5）RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配置, 默认值为 33554432B ,即 32M；（6）主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列( Deque )中, RecordAccumulator 内部为每个分区都维护了一个双端队列,即 Deque&lt;ProducerBatch&gt;。消息写入缓存时,追加到双端队列的尾部；（7）Sender 读取消息时,从双端队列的头部读取；（8）ProducerBatch是指一个消息批次; 与此同时,会将较小的 ProducerBatch凑成一个较大ProducerBatch ,也可以减少网络请求的次数以提升整体的吞吐量；（9）ProducerBatch大小和batch.size参数也有着密切的关系；（10）当一条消息(ProducerRecord ) 流入RecordAccumulator 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个ProducerBatch (如果没有则新建),查看ProducerBatch 中是否还可以写入这个ProducerRecord,如果可以写入,如果不可以则需要创建一个新的 Producer Batch；（11）在新建ProducerBatch 时评估这条消息的大小是否超过batch.size 参数大小, 如果不超过, 那么就以batch.size参数的大小来创建ProducerBatch；（12）Sender 从 RecordAccumulator 获取缓存的消息之后,会进一步将&lt;分区,Deque&lt;Producer Batch&gt;&gt;的形式转变成&lt;Node,List&lt; ProducerBatch&gt;的形式,其中 Node 表示 Kafka 集群 broker 节点；（13）对于网络连接来说,生产者客户端是与具体 broker 节点建立的连接,也就是向具体的 broker 节点发送消息,而并不关心消息属于哪一个分区；（14）而对于 KafkaProducer 的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I/O 层面的转换；（15）在转换成&lt;Node, List&lt;ProducerBatch&gt;&gt;的形式之后, Sender 会进一步封装成&lt;Node,Request&gt; 的形式, 这样就可以将 Request 请求发往各个 Node 了,这里的 Request 是 Kafka 各种协议请求；（16）请求在从sender 线程发往 Kafka 之前还会保存到 InFlightRequests中,InFlightRequests 保存对象的具体形式为 Map&lt;Nodeld, Deque&lt;request&gt;&gt;,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。 重要的生产者参数1234567（1）max.request.size：这个参数用来限制生产者客户端能发送的消息的最大值,默认值为 1048576B ,即 lMB 一般情况下,这个默认值就可以满足大多数的应用场景了。（2）compression.type：用来指定消息的压缩方式,默认值为“none &quot;,即默认情况下,消息不会被压缩还可以配置为 &quot;gzip&quot;,&quot;snappy&quot; 和 &quot;lz4&quot;，服务端也有压缩参数，先解压，再压缩对消息进行压缩可以极大地减少网络传输、降低网络 I/O,从而提高整体的性能。（3）retries 和 retry.backoff.ms：retries 参数用来配置生产者重试的次数,默认值为 0,即在发生异常的时候不进行任何重试动作。重试还和另一个参数 retry.backoff.ms 有关,这个参数的默认值为 100,它用来设定两次重试之间的时间间隔,避免无效的频繁重试。（4）batch.size：每个 Batch 要存放 batch.size 大小的数据后,才可以发送出去。比如说 batch.size 默认值是 16KB,那么里面凑够 16KB 的数据才会发送。（5）linger.ms：用来指定生产者发送 ProducerBatch 之前等待更多消息( ProducerRecord )加入ProducerBatch 时间,默认值为 0。（6）enable.idempotence：是否开启幂等性功能,详见后续原理加强。（7）partitioner.classe：用来指定分区器,默认:org.apache.kafka. internals.DefaultPartitioner --》用hashcode分。 2.消费者API 一个正常的消费逻辑需要具备以下几个步骤： 配置消费者客户端参数 创建相应的消费者实例 订阅主题 拉取消息并消费 提交消费位移offest 关闭消费者实例 subscribe重载方法： 前面两种通过集合的方式订阅一到多个topic Public 、void、subscribe（collection、topics、ConsumerRebalanceListenerlistener） Public、void、subscribe(collectiontopics) 后两种主要是采用正则的方式订阅一到多个topics public voidSubscribe（Pattern pattern，ConsumerRebalancelistener listener）Publicviod void subscribe(Patternopattern) 正则方式订阅主题(只要是tpc数字的形式，三位数字以内如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅。在之后的过程中，如果有人又创建了新的主题，并且主题名字与正表达式相匹配那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并可以处理不同的类型，那么这种订阅方式就很有效。利用正则表达式订阅主题，可实现动态订阅； assign订阅主题消费者不仅可以通过KafkaConsumersubscribe()方法订阅主题，还可直接订阅某些主题的指定分区:在KafkaConsumer中提供了assign方法来实现这些功能，此方法的具体定义如下:public void assign(Collectionpartitions);这个方法只接受参数partitions用来指定需要订阅的分区集合示例如下:consumer.assign(Arrays.asList(new TopicPartition (“tpc_1”,0),new TopicPartition(“tpc 2,1))); subscribe与assign的区别 通过subscribe0)方法订阅主题具有消费者自动再均衡功能:在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整以实现消费负载均衡及故障自动转移。 assign方法订阅分区时，是不具备消费者自动均衡的功能的;其实这一点从assign0方法参数可以看出端倪，两种类型subscribe都有 ConsumerRebalanceListener类型参数的方法。而assign()方法却没有。取消订阅 可以使用KafkaConsumer中的unsubscribe)方法采取消主题的订阅.这个方法既可以取消通过subscribe(Collection)方式实现的订阅; 也可以取消通过subscribe(Pattem)方式实现的订阅，还可以取消通过assign(Collection)方式实现的订阅。 如果将subscribe(Collection)或assign(Collection)集合参数设置为空集合，作用与unsubscribe0)方法相同。如下示例中三行代码的效果相同: consumer.unsubscribe0;consumer.subscribe(new ArrayListO)consumer.assign(new ArrayListO)； 消息的消费模式示例代码片段 3.Topic管理API一般情况下，我们习惯用kafka-topic.sh来进行管理主题，如果需要将管理类的功能集成用到公司内部系统中，就需要用到以API进行实现。这种调用API方式实现管理主要利用KafkaAdminClient工具类。 KafkaAdminClient不仅可以用来管理broker，配置和ACI（Access Control List，管理主题）它提供了以下方法：12345678创建主题：CreateTopics（Collecnoo&lt;New Topic&gt;）new topics)删除主题：DeleteTopicsResult delete Topres(Collecnoo&lt;String&gt;topics)列出所有可用主题：ListTopicsResult listTopics()查看主题的信息：DescribeTopicsRrsult(describeTopics(Collection&lt;String&gt;topicNames)查询配置信息：DescribeConfigsResult describeConfigs(Collection&lt;ConfigResource&gt;resources)修改配置信息：AlterConfigsResultalterConfigs(Map&lt;ConfigResource Config&gt;configs)增加分区：CreatePartionsResult createPartition(Map&lt;String.New[artitions&gt;new Partitions)构建一个KafkaAdminClient AdminClient adminClient=KafkaAdminClient.create(props)； 列出主题ListTopicsResult listTopicsResult&#x3D;adminClient.listTopics();Settopic&#x3D;listTopicsResult.names().get();System.out.rintln(topics); 查看主题信息123456DescribeTopicsResult describeTopicsResult=adminClient.describeTopics(Arrays.asList(“tpc_4”,”tpc_3”));Map&lt;String, TopicDescription&gt;res=describeTopicsResult.all().get();Set&lt;String&gt;ksets=res.keySet();for(String k : ksets)&#123; System.out.[rintln(res.get(k));&#125; 创建主题123456789101112131415161718192021222324252627282930313233//参数配置Properties props = new Properties();Props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node:9092);Props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000);//创建admin client对象AdminClient adminClient=KafkaAdminClient.create(props）；//由服务端controller自行分配分区及副本所生brokerNewTopic tpc_3=new NewTopic(“tpc_3”,2.(shot)1);//手动制定分区及副本的broker分配HashMap&lt;IInteger,List&lt;Integer&gt;&gt;replicaAssignments=new HashMap&lt;&gt;();//分区0，分配到broker（），broker1replicaAssignments.put(0,Arrays,asList(0,1));//分区1，分配到broker（），broker2replicaSssignments.put(0,Arrays.asList(0,1));NewTopic tpc_4=new NewTopic(“tpc_4”,re[licaAssignments);AdminClient adminClient=KafkaAdminClient.create(props);//由服务端controller 自行分配分区及副本所在brokerN额外Topictpc_3=newNewTopic(“tpc3”,2,(shot)1);//手动指定分区及副本的broker分配HashMap&lt;Integer,List&lt;Integer&gt;&gt;replicaAssignments=new HashMap&lt;&gt;();//分区0，分配到broker（），broker1replicaAssignments.put(0,Arrays.asList(0,1));//分区1，分配到broker（）broker2replicaAssignments.put(0,Arrays.asList(0,1))NewTopic tpc_4=new NewTopic(“tpc_4”,replicaAssignments);CreateTo[icsResult result=adminClient.createTopics(Arrays.asList(tpc_3.tpc4));//从future中等待服务端返回try&#123; result.all().get();&#125;catch(Exception e)&#123;e.printStackTrace();&#125;adminClient.close(); 代码示例","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Kafka基础环境配置","slug":"Kafka基础环境配置","date":"2022-06-15T07:43:33.219Z","updated":"2022-06-15T13:12:05.565Z","comments":true,"path":"2022/06/15/Kafka基础环境配置/","link":"","permalink":"http://example.com/2022/06/15/Kafka%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"","text":"本文主要使用Hexo与Github进行个人blog的搭建Hexo官网：HexoGithub官网：Github 环境介绍本地环境为: Window10系统、Linux虚拟机注意：本文配置与Spark基础环境配置相同，若Spark基础环境配置已配置，请直接观看下一文章 开始搭建1.基础环境在开始配置前，需要检查虚拟机主机名、hosts映射、关闭防火墙、免密登录、同步时间等操作 （1）编辑主机名（三台机器）123456789#查看系统主机名(三台主机)$ cat /etc/hostname#在三台主机上更改主机名#在 master 主节点$ echo &quot;master&quot; &gt;/etc/hostname #在 slave1 节点 $ echo &quot;slave1&quot; &gt;/etc/hostname #在 slave2 节点$ echo &quot;slave2&quot; &gt;/etc/hostname （2）hosts映射12345678910111213#查看系统映射$ cat /etc/hosts#编辑 /etc/hosts 文件$ vim /etc/hosts#内容修改为 （三台主机内容一致）127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 master 192.168.88.136 slave1 192.168.88.137 slave2 （3）关闭防火墙1234#关闭防火墙$ systemctl stop firewalld.service#禁止防火墙开启自启$ systemctl disable firewalld.service （4）免密登录123456#master 生成公钥私钥，四个回车即可$ ssh-keygen#master 配置免密登录到master slave1 slave2三台主机$ ssh-copy-id master $ ssh-copy-id slave1 $ ssh-copy-id slave2 （5）时间同步123456#安装 ntp$ yum install ntp -y #设置 ntp 开机自启动$ systemctl enable ntpd &amp;&amp; systemctl start ntpd#三台主机分别运行以下命令$ ntpdate ntp4.aliyun.com 2.JDK安装###（1）下载安装包本文使用的 JDK 是1.8版本jdk1.8安装包下载注意：下载的是后缀为 .tar.gz 的包 （2）在主机 master 上安装 JDK1234567891011#编译环境软件安装目录$ mkdir -p /export/server#上传本地下载好的jdk-8u241-linux-x64.tar.gz上传到/export/server/目录下 并解压文件$ tar -zxvf jdk-8u241-linux-x64.tar.gz#配置环境变量$ vim /etc/profile#在文件内添加如下内容# jdk 环境变量 export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.ja 12345#重新加载环境变量文件$ source /etc/profile#查看 java 版本号$ java -version#出现 java version &quot;1.8.0_241&quot; 表示安装成功 （3）分发12345678910#master 节点将 java 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ $ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/#配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s jdk1.8.0_241/ jdk#重新加载环境变量文件$ source /etc/profile 3.Hadoop安装（1）下载安装包本文使用的 hadoop 是3.3.0版本hadoop3.3.0安装包下载注意：下载的是后缀为 .tar.gz的包 （2）在主机 master 上安装 hadoop123456789101112131415#上传本地下载好的 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件$ tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz#修改配置文件,进入到 hadoop 目录下$ cd /export/server/hadoop-3.3.0/etc/hadoop#编辑 hadoop-env.sh 文件$ vim hadoop-env.sh#文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root 12345678910111213141516171819202122232425262728293031323334353637#修改 core-site.xml 文件 $ vim core-site.xml #添加如下内容&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; 1234567#修改 hdfs-site.xml 文件 $ vim hdfs-site.xml &lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:9868&lt;/value&gt;&lt;/property&gt; 1234567891011121314151617181920212223242526272829303132333435#修改 mapred-site.xml 文件$ vim mapred-site.xml#添加如下内容&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt;&lt;/property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt; 12345678910111213141516171819202122232425262728293031323334353637#修改 yarn-site.xml 文件$ vim yarn-site.xml#添加如下内容&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 123456#修改 workers 文件$ vim workers#将 workers 里的 localhost 删除，添加如下内容master slave1 slave2 （3）分发1234#master 节点将 hadoop 传输到 slave1 和 slave2$ cd /export/server$ scp -r hadoop-3.3.0 root@slave1:$PWD$ scp -r hadoop-3.3.0 root@slave2:$PWD 123456#将 hadoop 添加到环境变量vim /etc/profile#在文件内添加如下内容# hadoop 环境变量 export HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 123456789101112131415#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s hadoop-3.3.0/ hadoop#重新加载环境变量文件$ source /etc/profile#在 master 主节点进行 Hadoop 集群启动 格式化 namenode（只有首次启动需要格式化）$ hdfs namenode -format#等待初始化完成后，使用脚本一键起动$ start-all.sh #起动后，输入jps查看进程号$ jps#进程查看完毕后可进入到 WEB 界面#HDFS集群的界面网站是:http://master:9870/#YARN集群的界面网站是:http://master:9870/ 4.安装zookeeper（1）下载安装包本文使用的 zookeeper 是3.7.0版本zookeeper3.7.0安装包下载注意：下载的是后缀为 .tar.gz 的包,安装包需要3.7版本网上，否者后续Kafka配置会出现问题 （2）在主机 master 上安装 zookeeper12345678#上传本地下载好的 apache-zookeeper-3.7.0-bin.tar.gz 上传到 /export/server 并解压文件$ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz#修改配置文件,进入到 /export/server 目录下$ cd /export/server/#在 /export/server 目录下创建 zookeeper 软连接$ ln -s apache-zookeeper-3.7.0-bin/ zookeeper#进入到 zookeeper 目录下$ cd zookeeper 12345678910111213141516#进入到 zookeeper 下的 conf 文件内$ cd /export/server/zookeeper/conf/ #将 zoo_sample.cfg 文件复制为新文件 zoo.cfg$ cp zoo_sample.cfg zoo.cfg#在 zoo.cfg 文件内添加如下内容#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888 1234567#进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件,将 1 写入进去$ cd /export/server/zookeeper/zkdata$ mkdir myid$ echo &#x27;1&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字1即为成功 （3）分发12345#master 节点将 zookeeper 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/zookeeper/ slave1:$PWD$ scp -r /export/server/zookeeper/ slave2:$PWD#推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和 3 123456#在 slave1 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;2&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字2即为成功 123456#在 slave2 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;3&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字3即为成功 123456#将 zookeeper 添加到环境变量vim /etc/profile#在文件内添加如下内容# zookeeper 环境变量 export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin 12345678910#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#重新加载环境变量文件$ source /etc/profile#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本$ cd /export/server/zookeeper/bin$ zkServer.sh start#查看 zookeeper 的状态$ zkServer.sh status#也可以通过jps查看zookeeper的进程$ jps 以上,就是Kafka基础环境的配置,接下来会带来 Kafka命令行操作","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Spark HA & Yarn配置","slug":"Spark HA & Yarn配置","date":"2022-05-19T08:19:25.354Z","updated":"2022-05-19T10:44:31.962Z","comments":true,"path":"2022/05/19/Spark HA & Yarn配置/","link":"","permalink":"http://example.com/2022/05/19/Spark%20HA%20&%20Yarn%E9%85%8D%E7%BD%AE/","excerpt":"","text":"本文着重描述 Spark HA 模式和 Spark on Yarn 模式注意：以下操作是基于Spark local &amp; stand-alone配置,若未操作,请移步到[Spark local &amp; stand-alone配置](..&#x2F;..&#x2F;17&#x2F;Spark local &amp; stand-alone配置)进行配置 1.Spark HA 模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题；aster故障后，集群就不可用。在HA模式下当Active的Master出现故障时,另外的一个Standby Master会被选举出来 （1）更改 Spark 配置文件内容123#在虚拟机 master 上进入到/export/server/spark/conf/目录下$ cd /export/server/spark/conf/#向conf文件下的 spark-env.sh 文件添加内容 **在进行添加内容之前,要删除或者注释掉 export SPARK_MASTER_HOST&#x3D;master **代码位于文件83行,要显示行数可通过:set nu查看 123456$ vim spark-env.sh#在文件末添加如下内容SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER - Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha&quot; # spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 # 指定Zookeeper的连接地址 # 指定在Zookeeper中注册临时节点的路径 （2）分发1234#将更改的文件到slave1、slave2上$ cd /export/server/spark/conf/$ scp spark-env.sh slave1:/export/server/spark/conf/$ scp spark-env.sh slave2:/export/server/spark/conf/ （3）开启三台主机启动Zookeeper 1234567#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本$ cd /export/server/zookeeper/bin$ zkServer.sh start#查看 zookeeper 的状态$ zkServer.sh status#也可以通过jps查看zookeeper的进程$ jps master启动hadoop 12345#在 master 主节点启动hadoop#使用脚本一键起动$ start-all.sh #起动后，输入jps查看进程号$ jps master主节点启动master和worker进程 1234#在 master 主节点进入到 /export/server/spark/sbin/ 目录下$ cd /export/server/spark/sbin/ #开启master和worker进程$ sh start-all.sh 访问 WebUI 界面http://master:8080/出现访问不成功的时候可能是端口被占用，将8080换为8081、8082….进行尝试 访问WebUI界面，查看slave1和slave2的状态，可以看到slave1的状态为ALIVE，slave2状态为STANDBY 此时 kill 掉 master 上的 master 的 16551 进程号，通过jps查看端口是否删除 123#kill 进程16551$ kill -9 16551$ jps 注意:删除的进程号,需要是主机master通过jps查看后的Master进程号刷新node2WebUI界面，发现其状态由STANDBY变为ALIVE此时master不能在访问,需要更改端口号才能访问,逐次增加1进行尝试 2.Spark on Yarn 模式在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端 Spark在YARN上的角色 Master角色由YARN的ResourceManager担任 Worker角色由YARN的NodeManager担任. Driver角色运行在YARN容器内或提交任务的客户端进程中 Executor运行在YARN提供的容器内 （1）更改 Spark 配置文件内容12345678#在虚拟机 master 上进入到/export/server/spark/conf/目录下$ cd /export/server/spark/conf/#向conf文件下的 spark-env.sh 文件添加内容$ vim spark-env.sh#在文件末添加如下内容## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop （2）运行在YARN上运行spark 123#可以直接运行以下命令$ /export/server/spark/bin/pyspark --master yarn#出现Spark表示,表明可以在YARN集群上运行spark client模式测试 1234#进入到 /export/server/spark/$ cd /export/server/spark/#进行 client 测试$ /bin/spark-submit --master yarn --deploy-mode client -- driver-memory 512m --executor-memory 512m --num-executors 1 --total- executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 cluster模式测试 1234#进入到 /export/server/spark/$ cd /export/server/spark/#进行 cluster 测试$ /bin/spark-submit --master yarn --deploy-mode cluster -- driver-memory 512m --executor-memory 512m --num-executors 1 --total- executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 若要查看它的具体运行情况，需要开启hadoop的历史服务 1234#进入到 /export/server/hadoop/sbin$ cd /export/server/hadoop/sbin#启动 hadoop 历史服务器$ ./mr-jobhistory-daemon.sh start historyserver 启动历史服务器后,访问WebUI界面，查看client模式下的运行状况 启动历史服务器后,访问WebUI界面，查看cluster模式下的运行状况 以上,就是 Spark HA &amp; Yarn配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Spark local & stand-alone配置","slug":"Spark local& stand-alone配置","date":"2022-05-17T12:18:47.360Z","updated":"2022-06-15T12:49:23.256Z","comments":true,"path":"2022/05/17/Spark local& stand-alone配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群) Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试 Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在linux系统之上 hadoop YARN 模式:Spark中的各个角色运行在YARN的容器内部,并组成Spark集群环境,运行在yarn容器内 Spark中的各个角色运行在Kubernetes的容器内部,并组成Spark集群环境 本文着重描述 Spark local 模式和 Spark alone 模式1.Spark local 模式Spark local 模式是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时的环境Spark由四类角色组成整个Spark的运行环境: Master角色，管理整个集群的资源 Worker角色，管理单个服务器的资源 Driver角色，管理单个Spark任务在运行的时候的工作 Executor角色，单个任务运行的时候的工作者 注意：以下操作需要完成 Spark 基础环境配置。具体配置移步到Spark基础环境配置 （1）Anaconda安装Anaconda安装包下载，不建议去官网下载，官网下载太慢了，推荐下载地址Anaconda清华镜像站注意：下载的是 Anaconda3-2021.05-Linux-x86_64.sh 后缀为 .sh 的安装包 1234#上传本地下载好的 Anaconda 安装包上到 /export/server/ 目录下进行安装$ cd /export/server#执行文件$ sh Anaconda3-2021.05-Linux-x86_64.sh 1)在遇到 Do you accept the license terms? [yes|no]时，选择yes2)在上述命令回车后，会让你选择你想要安装的路径，统一安装在&#x2F;export&#x2F;server&#x2F;anaconda3下3)等待执行完毕后，在新的[yes|no]选择界面选择yes；随后exit退出重新登录即可看到base，代表着安装完成 1234#创建虚拟环境pyspark，基于Python 3.8$ conda create -n pyspark python=3.8#切换到虚拟环境内$ conda activate pyspark 4)看到(pyspark)表示成功 12#在虚拟环境内安装包$ pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple （2）Spark安装本文使用的 spark 是3.2.0版本spark 3.2.0 安装包下载注意：下载要注意hadoop的版本，同时选择后缀为 .tar.gz的安装包本文使用的安装包是spark-3.2.0-bin-hadoop3.2.tgz 1234567891011121314#把本地下载好的spark-3.2.0-bin-hadoop3.2.tgz安装包上传到 /export/server 并解压$ cd /export/server/$ tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/#建立软连接$ ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark#编辑环境变量$ vim /etc/profile#在文件末添加以下内容#SPARK_HOME export SPARK_HOME=/export/server/spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 1234567#编辑.bashrc文件，添加java和pyspark的Home值$ vim .bashrc#添加以下内容#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 1234567#重新加载环境变量文件$ source /etc/profile$ source ~/.bashrc#进入 /export/server/anaconda3/envs/pyspark/bin/ 文件夹$ cd /export/server/anaconda3/envs/pyspark/bin/#开启 Spark$ ./pyspark 看到 pyspark 表示成功 123#测试运行基于python的spark解释器环境，在下方运行以下代码$ sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()#结果出来后,查看WebUI界面:http://master:4040/ 2.Spark alone 模式tand-alone集群模式中，Spark的各个角色以独立进程的形式存在，并组成Spark集群环境StandAlone集群在进程上主要有三类: 主节点Master进程：Master角色，管理整个集群资源，并托管各个任务的Driver 从节点Workers：Worker角色，管理每个机器的资源，分配对应资源来运行Executor（Task） 历史服务器HistoryServer：在Spark Application运行完成以后，保存事件日志数据至HDFS （1）Spark alone 模式下的新配置前提:三台虚拟机全部安装 Anaconda参考Spark（local）模式下的Anaconda的安装文档，在slave1、slave2完成对Anaconda的安装 12#master 节点节点进入 /export/server/spark/conf 修改以下配置文件$ cd /export/server/spark/conf 12345678#将文件 workers.template 改名为 workers，并配置文件内容$ mv workers.template workers#修改 workers 文件$ vim workers#将 workers 里的 localhost 删除，添加如下内容master slave1 slave2 12345678910111213141516171819202122232425262728#将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容$ mv spark-env.sh.template spark-env.sh#修改 spark-env.sh 文件$ vim spark-env.sh#文件最后添加以下内容## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=master # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=&quot;- Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - Dspark.history.fs.cleaner.enabled=true&quot; 1234567#开启hadoop服务$ start-all.sh#在HDFS上创建程序运行历史记录存放文件夹$ hadoop fs -mkdir /sparklog#已存在会显示File exists,属于正常状况#给sparklog添加权限$ hadoop fs -chmod 777 /sparklog 12345678910111213#重新进入到 /export/server/spark/conf/ 目录下$ cd /export/server/spark/conf/#将spark-defaults.conf.template 文件改为spark-defaults.conf$ mv spark-defaults.conf.template spark-defaults.conf#编辑 spark-defaults.conf$ vim spark-defaults.conf#文件最后添加以下内容# 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://master:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true 12#将log4j.properties.template文件改为log4j.properties$ mv log4j.properties.template log4j.properties 配置 log4j.properties 文件将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为log4j.rootCategory&#x3D;WARN, console （2）分发123456#master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上$ cd /```bash#在 slave1 节点上$ ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark#重新加载环境变量$ source /etc/profile 1234567#在 slave2 节点上$ ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark#重新加载环境变量$ source /etc/profile```export/server$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave1:$PWD$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave2:$PWD 在slave1 和 slave2 上做软连接 完成上述操作后，启动历史服务器 12345#在 master 主机上进入 /export/server/spark/sbin 文件目录下$ cd /export/server/spark/sbin#启动历史服务器$ ./start-history-server.sh#访问 WebUI 界面:http://master:18080/ 123456#启动Spark的Master和Worker进程$ sh start-all.sh#通过jps查看进程是否开启master和worker进程$ jps 在查看到 Master 和 Worker 进程后表明成功启动#访问 WebUI界面:http://master:8080/ 以上,就是 Spark local&amp; stand-alone配置,接下来会带来 Spark HA &amp; Yarn配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]},{"title":"Spark基础环境配置","slug":"Spark基础环境配置","date":"2022-05-17T12:18:46.617Z","updated":"2022-06-15T09:39:40.334Z","comments":true,"path":"2022/05/17/Spark基础环境配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"","text":"本文主要使用Hexo与Github进行个人blog的搭建Hexo官网：HexoGithub官网：Github 环境介绍本地环境为: Window10系统、Linux虚拟机注意：本文配置与Kafka基础环境配置相同，若Kafka基础环境配置已配置，请直接观看下一文章 开始搭建1.基础环境在开始配置前，需要检查虚拟机主机名、hosts映射、关闭防火墙、免密登录、同步时间等操作 （1）编辑主机名（三台机器）123456789#查看系统主机名(三台主机)$ cat /etc/hostname#在三台主机上更改主机名#在 master 主节点$ echo &quot;master&quot; &gt;/etc/hostname #在 slave1 节点 $ echo &quot;slave1&quot; &gt;/etc/hostname #在 slave2 节点$ echo &quot;slave2&quot; &gt;/etc/hostname （2）hosts映射12345678910111213#查看系统映射$ cat /etc/hosts#编辑 /etc/hosts 文件$ vim /etc/hosts#内容修改为 （三台主机内容一致）127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 master 192.168.88.136 slave1 192.168.88.137 slave2 （3）关闭防火墙1234#关闭防火墙$ systemctl stop firewalld.service#禁止防火墙开启自启$ systemctl disable firewalld.service （4）免密登录123456#master 生成公钥私钥，四个回车即可$ ssh-keygen#master 配置免密登录到master slave1 slave2三台主机$ ssh-copy-id master $ ssh-copy-id slave1 $ ssh-copy-id slave2 （5）时间同步123456#安装 ntp$ yum install ntp -y #设置 ntp 开机自启动$ systemctl enable ntpd &amp;&amp; systemctl start ntpd#三台主机分别运行以下命令$ ntpdate ntp4.aliyun.com 2.JDK安装（1）下载安装包本文使用的 JDK 是1.8版本jdk1.8安装包下载注意：下载的是后缀为 .tar.gz 的包 （2）在主机 master 上安装 JDK1234567891011#编译环境软件安装目录$ mkdir -p /export/server#上传本地下载好的jdk-8u241-linux-x64.tar.gz上传到/export/server/目录下 并解压文件$ tar -zxvf jdk-8u241-linux-x64.tar.gz#配置环境变量$ vim /etc/profile#在文件内添加如下内容# jdk 环境变量 export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.ja 12345#重新加载环境变量文件$ source /etc/profile#查看 java 版本号$ java -version#出现 java version &quot;1.8.0_241&quot; 表示安装成功 （3）分发12345678910#master 节点将 java 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ $ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/#配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s jdk1.8.0_241/ jdk#重新加载环境变量文件$ source /etc/profile 3.Hadoop安装（1）下载安装包本文使用的 hadoop 是3.3.0版本hadoop3.3.0安装包下载注意：下载的是后缀为 .tar.gz的包 （2）在主机 master 上安装 hadoop1234567891011121314#上传本地下载好的 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件$ tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz#修改配置文件,进入到 hadoop 目录下$ cd /export/server/hadoop-3.3.0/etc/hadoop#编辑 hadoop-env.sh 文件$ vim hadoop-env.sh#文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root 1234567891011121314151617181920212223242526272829303132333435#修改 core-site.xml 文件 $ vim core-site.xml #添加如下内容&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;hdfs-site.xmlmapred-site.xml&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; 12345678910111213141516171819202122232425262728#修改 mapred-site.xml 文件$ vim mapred-site.xml#添加如下内容&lt;!-- 设置MR程序默认运行模式： yarn 集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; 12345678#修改 hdfs-site.xml 文件$ vim hdfs-site.xml#添加如下内容&lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:9868&lt;/value&gt; &lt;/property&gt; 12345678910111213141516171819202122232425262728293031323334353637#修改 yarn-site.xml 文件$ vim yarn-site.xml#添加如下内容&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 123456#修改 workers 文件$ vim workers#将 workers 里的 localhost 删除，添加如下内容master slave1 slave2 （3）分发1234#master 节点将 hadoop 传输到 slave1 和 slave2$ cd /export/server$ scp -r hadoop-3.3.0 root@slave1:$PWD$ scp -r hadoop-3.3.0 root@slave2:$PWD 123456#将 hadoop 添加到环境变量vim /etc/profile#在文件内添加如下内容# hadoop 环境变量 export HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 123456789101112131415#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#配置完成后，在 master slave1 和slave2 三台主机创建软连接$ cd /export/server $ ln -s hadoop-3.3.0/ hadoop#重新加载环境变量文件$ source /etc/profile#在 master 主节点进行 Hadoop 集群启动 格式化 namenode（只有首次启动需要格式化）$ hdfs namenode -format#等待初始化完成后，使用脚本一键起动$ start-all.sh #起动后，输入jps查看进程号$ jps#进程查看完毕后可进入到 WEB 界面#HDFS集群的界面网站是:http://master:9870/#YARN集群的界面网站是:http://master:9870/ 4.安装zookeeper（1）下载安装包本文使用的 zookeeper 是3.7.0版本zookeeper3.7.0安装包下载注意：下载的是后缀为 .tar.gz 的包,安装包需要3.7版本网上，否者后续spark配置会出现问题 （2）在主机 master 上安装 zookeeper12345678#上传本地下载好的 apache-zookeeper-3.7.0-bin.tar.gz 上传到 /export/server 并解压文件$ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz#修改配置文件,进入到 /export/server 目录下$ cd /export/server/#在 /export/server 目录下创建 zookeeper 软连接$ ln -s apache-zookeeper-3.7.0-bin/ zookeeper#进入到 zookeeper 目录下$ cd zookeeper 12345678910111213141516#进入到 zookeeper 下的 conf 文件内$ cd /export/server/zookeeper/conf/ #将 zoo_sample.cfg 文件复制为新文件 zoo.cfg$ cp zoo_sample.cfg zoo.cfg#在 zoo.cfg 文件内添加如下内容#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888 1234567#进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件,将 1 写入进去$ cd /export/server/zookeeper/zkdata$ mkdir myid$ echo &#x27;1&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字1即为成功 （3）分发12345#master 节点将 zookeeper 传输到 slave1 和 slave2$ cd /export/server$ scp -r /export/server/zookeeper/ slave1:$PWD$ scp -r /export/server/zookeeper/ slave2:$PWD#推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和 3 123456#在 slave1 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;2&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字2即为成功 123456#在 slave2 节点上$ cd /export/server/zookeeper/zkdatas/$ echo &#x27;3&#x27; &gt; myid#查看是否成功写入$ vim myid#出现数字3即为成功 123456#将 zookeeper 添加到环境变量vim /etc/profile#在文件内添加如下内容# zookeeper 环境变量 export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin 12345678910#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）#重新加载环境变量文件$ source /etc/profile#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本$ cd /export/server/zookeeper/bin$ zkServer.sh start#查看 zookeeper 的状态$ zkServer.sh status#也可以通过jps查看zookeeper的进程$ jps 以上,就是Spark基础环境的配置,接下来会带来 Spark local&amp; stand-alone配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"主题","slug":"主题","permalink":"http://example.com/tags/%E4%B8%BB%E9%A2%98/"},{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"}]}
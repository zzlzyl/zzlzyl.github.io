<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Spark使用</title>
  
  <subtitle>让你感受专业的配置</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-15T13:11:31.908Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>zzl</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka命令行操作 </title>
    <link href="http://example.com/2022/06/15/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"/>
    <id>http://example.com/2022/06/15/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/</id>
    <published>2022-06-15T11:58:03.012Z</published>
    <updated>2022-06-15T13:11:31.908Z</updated>
    
    <content type="html"><![CDATA[<p><strong>注意：以下操作需要完成 Kafka 基础环境配置。具体配置移步到<a href="../../15/Kafka%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">Kafka基础环境配置</a></strong></p><h2 id="1-Kafka安装"><a href="#1-Kafka安装" class="headerlink" title="1.Kafka安装"></a>1.Kafka安装</h2><p>本文使用的 Kafka 是2.11-2.0.0版本<br><a href="https://kafka.apache.org/downloads">Kafka 3.2.0 安装包下载</a><br><strong>注意：下载要注意 Kafka 的版本，同时选择后缀为 .tar.gz的安装包</strong><br>**本文使用的安装包是 kafka_2.11-2.0.0.tgz **</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把本地下载好的 kafka_2.11-2.0.0.tgz安装包上传到 /export/server 并解压</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/</span><br><span class="line">$ tar -zxvf kafka_2.11-2.0.0.tgz -C /export/server/</span><br><span class="line"><span class="comment">#建立软连接</span></span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/kafka_2.11-2.0.0.tgz /export/server/kafka</span><br><span class="line"><span class="comment">#master 节点节点进入 /export/server/kafka/config 修改以下配置文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/kafka/config</span><br></pre></td></tr></table></figure><h4 id="编辑文件-server-properties"><a href="#编辑文件-server-properties" class="headerlink" title="编辑文件 server.properties"></a>编辑文件 server.properties</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ vim server.properties</span><br><span class="line"><span class="comment">#21 行内容 broker.id=0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复</span></span><br><span class="line"><span class="comment">#（注：此处为master节点，不用修改）</span></span><br><span class="line"><span class="variable">$broker</span>.<span class="built_in">id</span>=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#31 行内容 #listeners=PLAINTEXT://:9092 取消注释，内容改为</span></span><br><span class="line">$ listeners=PLAINTEXT://master:9092</span><br><span class="line"></span><br><span class="line"><span class="comment">#59 行内容 log.dirs=/tmp/kafka-logs 为默认日志文件存储的位置，改为</span></span><br><span class="line"><span class="variable">$log</span>.<span class="built_in">dirs</span>=/export/server/data/kafka-log</span><br><span class="line"></span><br><span class="line"><span class="comment">#63 行内容为 num.partitions=1 是默认分区数</span></span><br><span class="line">$ num.partitions=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#121 行内容 zookeeper.connect=localhost:2181 修改为</span></span><br><span class="line"><span class="variable">$zookeeper</span>.connect=master:2181,slave1:2181,slave2:2181</span><br><span class="line"></span><br><span class="line"><span class="comment">#126 行内容 group.initial.rebalance.delay.ms=0 修改为</span></span><br><span class="line">$ group.initial.rebalance.delay.ms=3000</span><br></pre></td></tr></table></figure><h2 id="2-分发"><a href="#2-分发" class="headerlink" title="2.分发"></a>2.分发</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点分发 kafka 安装文件夹 到 slave1 和 slave2 上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r /export/server/kafka_2.11-2.0.0.tgz/ slave1:<span class="variable">$PWD</span></span><br><span class="line">$ scp -r /export/server/kafka_2.11-2.0.0.tgz/ slave2:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置 kafka 环境变量，master、slave1、slave2都需要进行操作</span></span><br><span class="line"><span class="comment">#vim /etc/profile</span></span><br><span class="line"><span class="comment">#文件最后添加以下内容</span></span><br><span class="line"><span class="comment"># kafka 环境变量 </span></span><br><span class="line">$ <span class="built_in">export</span> KAFKA_HOME=/export/server/kafka </span><br><span class="line">$ <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#三台机器操作完成重新加载环境变量</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave1 节点上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/kafka_2.11-2.0.0/ kafka</span><br><span class="line"><span class="comment">#进入 /export/server/kafka/config 修改以下配置文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/kafka/config</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim server.properties</span><br><span class="line"><span class="comment">#将文件 server.properties 的第 21 行的 broker.id=0 修改为 </span></span><br><span class="line">$ broker.id=1 </span><br><span class="line"><span class="comment">#将文件 server.properties 的第 31 行的 listeners=PLAINTEXT://master:9092 修改为</span></span><br><span class="line">$ listeners=PLAINTEXT://slave1:9092</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave2 节点上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/kafka_2.11-2.0.0/ kafka</span><br><span class="line"><span class="comment">#进入 /export/server/kafka/config 修改以下配置文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/kafka/config</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim server.properties</span><br><span class="line"><span class="comment">#将文件 server.properties 的第 21 行的 broker.id=0 修改为 </span></span><br><span class="line">$ broker.id=2 </span><br><span class="line"><span class="comment">#将文件 server.properties 的第 31 行的 listeners=PLAINTEXT://master:9092 修改为</span></span><br><span class="line">$ listeners=PLAINTEXT://slave2:9092</span><br></pre></td></tr></table></figure><p><strong>以上操作完成回到 master 节点</strong></p><h4 id="启动-kafka"><a href="#启动-kafka" class="headerlink" title="启动 kafka"></a>启动 kafka</h4><p><strong>注意：启动 kafka 需要启动 zookeeper</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动 kafka,三台机器同时执行</span></span><br><span class="line">$ kafka-server-start.sh -daemon /export/server/kafka/config/server.properties</span><br><span class="line"><span class="comment">#启动完成后通过jps查看其状态</span></span><br></pre></td></tr></table></figure><p><img src="/../images/001.png"></p><h4 id="设置脚本便于启动三台机器"><a href="#设置脚本便于启动三台机器" class="headerlink" title="设置脚本便于启动三台机器"></a>设置脚本便于启动三台机器</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入到bin目录下</span></span><br><span class="line"><span class="variable">$cd</span> /root/bin</span><br><span class="line"><span class="comment"># 创建并编辑名为kafka-all.sh的脚本</span></span><br><span class="line">$ vim kafka-all.sh</span><br><span class="line"><span class="comment">#在文件内添加如下内容</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -eq 0 ] ;</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;please input param:start stop&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> = start  ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ing master&quot;</span></span><br><span class="line">ssh master <span class="string">&quot;source /etc/profile;kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..2&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ing slave<span class="variable">$&#123;i&#125;</span>&quot;</span></span><br><span class="line">ssh slave<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> = stop ];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ping master &quot;</span></span><br><span class="line">ssh master <span class="string">&quot;source /etc/profile;kafka-server-stop.sh&quot;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..2&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ping slave<span class="variable">$&#123;i&#125;</span>&quot;</span></span><br><span class="line">ssh slave<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;kafka-server-stop.sh&quot;</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h2 id="3-Kafka命令行操作"><a href="#3-Kafka命令行操作" class="headerlink" title="3.Kafka命令行操作"></a>3.Kafka命令行操作</h2><h4 id="（1）创建topic"><a href="#（1）创建topic" class="headerlink" title="（1）创建topic"></a>（1）创建topic</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kafka-configs.sh --create --topic tpc_1 --partitions 2 --replication-factor2 --zookeeper node1:2181</span><br></pre></td></tr></table></figure><h4 id="（2）删除topic"><a href="#（2）删除topic" class="headerlink" title="（2）删除topic"></a>（2）删除topic</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1:2181</span><br></pre></td></tr></table></figure><h4 id="（3）查看topic"><a href="#（3）查看topic" class="headerlink" title="（3）查看topic"></a>（3）查看topic</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看当前系统中的所有topic</span></span><br><span class="line">$ kafka-topics.sh --zookeeper node1:2181-list</span><br><span class="line"><span class="comment">#查看topic详细信息</span></span><br><span class="line">$ kafka-configs.sh --create --topic tpc_1 --zookeeper node1:2181--replication-factor0:1</span><br></pre></td></tr></table></figure><h4 id="（4）增加分区数"><a href="#（4）增加分区数" class="headerlink" title="（4）增加分区数"></a>（4）增加分区数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181</span><br></pre></td></tr></table></figure><h4 id="（5）动态配置topic参数"><a href="#（5）动态配置topic参数" class="headerlink" title="（5）动态配置topic参数"></a>（5）动态配置topic参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#通过管理命令，可以为已创建的topic增加，修改，删除 topic level 参数</span></span><br><span class="line"><span class="comment">#添加，修改配置参数</span></span><br><span class="line">$ kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip</span><br><span class="line"><span class="comment">#删除配置参数</span></span><br><span class="line">$ kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;注意：以下操作需要完成 Kafka 基础环境配置。具体配置移步到&lt;a href=&quot;../../15/Kafka%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE&quot;&gt;Kafka基础环境配置&lt;/a&gt;&lt;/s</summary>
      
    
    
    
    <category term="工具" scheme="http://example.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
    <category term="主题" scheme="http://example.com/tags/%E4%B8%BB%E9%A2%98/"/>
    
    <category term="搭建" scheme="http://example.com/tags/%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Kafka基础环境配置</title>
    <link href="http://example.com/2022/06/15/Kafka%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/06/15/Kafka%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</id>
    <published>2022-06-15T07:43:33.219Z</published>
    <updated>2022-06-15T13:12:05.565Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要使用Hexo与Github进行个人blog的搭建<br>Hexo官网：<a href="https://hexo.io/zh-cn/">Hexo</a><br>Github官网：<a href="https://github.com/">Github</a></p><h3 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h3><p>本地环境为: Window10系统、Linux虚拟机<br><strong>注意：本文配置与Spark基础环境配置相同，若Spark基础环境配置已配置，请直接观看下一文章</strong></p><hr><h1 id="开始搭建"><a href="#开始搭建" class="headerlink" title="开始搭建"></a>开始搭建</h1><h2 id="1-基础环境"><a href="#1-基础环境" class="headerlink" title="1.基础环境"></a>1.基础环境</h2><p>在开始配置前，需要检查虚拟机主机名、hosts映射、关闭防火墙、免密登录、同步时间等操作</p><h3 id="（1）编辑主机名（三台机器）"><a href="#（1）编辑主机名（三台机器）" class="headerlink" title="（1）编辑主机名（三台机器）"></a>（1）编辑主机名（三台机器）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看系统主机名(三台主机)</span></span><br><span class="line">$ <span class="built_in">cat</span> /etc/hostname</span><br><span class="line"><span class="comment">#在三台主机上更改主机名</span></span><br><span class="line"><span class="comment">#在 master 主节点</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;master&quot;</span> &gt;/etc/hostname </span><br><span class="line"><span class="comment">#在 slave1 节点 </span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;slave1&quot;</span> &gt;/etc/hostname </span><br><span class="line"><span class="comment">#在 slave2 节点</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;slave2&quot;</span> &gt;/etc/hostname</span><br></pre></td></tr></table></figure><h3 id="（2）hosts映射"><a href="#（2）hosts映射" class="headerlink" title="（2）hosts映射"></a>（2）hosts映射</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看系统映射</span></span><br><span class="line">$ <span class="built_in">cat</span> /etc/hosts</span><br><span class="line"><span class="comment">#编辑 /etc/hosts 文件</span></span><br><span class="line">$ vim /etc/hosts</span><br><span class="line"><span class="comment">#内容修改为 （三台主机内容一致）</span></span><br><span class="line">127.0.0.1 localhost localhost.localdomain localhost4 </span><br><span class="line">localhost4.localdomain4 </span><br><span class="line">::1 localhost localhost.localdomain localhost6 </span><br><span class="line">localhost6.localdomain6 </span><br><span class="line"></span><br><span class="line">192.168.88.135 master </span><br><span class="line">192.168.88.136 slave1 </span><br><span class="line">192.168.88.137 slave2</span><br></pre></td></tr></table></figure><h3 id="（3）关闭防火墙"><a href="#（3）关闭防火墙" class="headerlink" title="（3）关闭防火墙"></a>（3）关闭防火墙</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#关闭防火墙</span></span><br><span class="line">$ systemctl stop firewalld.service</span><br><span class="line"><span class="comment">#禁止防火墙开启自启</span></span><br><span class="line">$ systemctl <span class="built_in">disable</span> firewalld.service</span><br></pre></td></tr></table></figure><h3 id="（4）免密登录"><a href="#（4）免密登录" class="headerlink" title="（4）免密登录"></a>（4）免密登录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 生成公钥私钥，四个回车即可</span></span><br><span class="line">$ ssh-keygen</span><br><span class="line"><span class="comment">#master 配置免密登录到master slave1 slave2三台主机</span></span><br><span class="line">$ ssh-copy-id master </span><br><span class="line">$ ssh-copy-id slave1 </span><br><span class="line">$ ssh-copy-id slave2</span><br></pre></td></tr></table></figure><h3 id="（5）时间同步"><a href="#（5）时间同步" class="headerlink" title="（5）时间同步"></a>（5）时间同步</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装 ntp</span></span><br><span class="line">$ yum install ntp -y </span><br><span class="line"><span class="comment">#设置 ntp 开机自启动</span></span><br><span class="line">$ systemctl <span class="built_in">enable</span> ntpd &amp;&amp; systemctl start ntpd</span><br><span class="line"><span class="comment">#三台主机分别运行以下命令</span></span><br><span class="line">$ ntpdate ntp4.aliyun.com</span><br></pre></td></tr></table></figure><hr><h2 id="2-JDK安装"><a href="#2-JDK安装" class="headerlink" title="2.JDK安装"></a>2.JDK安装</h2><p>###（1）下载安装包<br>本文使用的 JDK 是1.8版本<br><a href="https://www.oracle.com/java/technologies/downloads/#java8">jdk1.8安装包下载</a><br><strong>注意：下载的是后缀为 .tar.gz 的包</strong></p><h3 id="（2）在主机-master-上安装-JDK"><a href="#（2）在主机-master-上安装-JDK" class="headerlink" title="（2）在主机 master 上安装 JDK"></a>（2）在主机 master 上安装 JDK</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译环境软件安装目录</span></span><br><span class="line">$ <span class="built_in">mkdir</span> -p /export/server</span><br><span class="line"><span class="comment">#上传本地下载好的jdk-8u241-linux-x64.tar.gz上传到/export/server/目录下 并解压文件</span></span><br><span class="line">$ tar -zxvf jdk-8u241-linux-x64.tar.gz</span><br><span class="line"><span class="comment">#配置环境变量</span></span><br><span class="line">$ vim /etc/profile</span><br><span class="line"><span class="comment">#在文件内添加如下内容</span></span><br><span class="line"><span class="comment"># jdk 环境变量 </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241 </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin </span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.ja</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment">#查看 java 版本号</span></span><br><span class="line">$ java -version</span><br><span class="line"><span class="comment">#出现 java version &quot;1.8.0_241&quot; 表示安装成功</span></span><br></pre></td></tr></table></figure><h3 id="（3）分发"><a href="#（3）分发" class="headerlink" title="（3）分发"></a>（3）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点将 java 传输到 slave1 和 slave2</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ </span><br><span class="line">$ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/</span><br><span class="line"><span class="comment">#配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）</span></span><br><span class="line"><span class="comment">#配置完成后，在 master slave1 和slave2 三台主机创建软连接</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server </span><br><span class="line">$ <span class="built_in">ln</span> -s jdk1.8.0_241/ jdk</span><br><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><hr><h2 id="3-Hadoop安装"><a href="#3-Hadoop安装" class="headerlink" title="3.Hadoop安装"></a>3.Hadoop安装</h2><h3 id="（1）下载安装包"><a href="#（1）下载安装包" class="headerlink" title="（1）下载安装包"></a>（1）下载安装包</h3><p>本文使用的 hadoop 是3.3.0版本<br><a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz">hadoop3.3.0安装包下载</a><br><strong>注意：下载的是后缀为 .tar.gz的包</strong></p><h3 id="（2）在主机-master-上安装-hadoop"><a href="#（2）在主机-master-上安装-hadoop" class="headerlink" title="（2）在主机 master 上安装 hadoop"></a>（2）在主机 master 上安装 hadoop</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上传本地下载好的 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件</span></span><br><span class="line">$ tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</span><br><span class="line"><span class="comment">#修改配置文件,进入到 hadoop 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/hadoop-3.3.0/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment">#编辑 hadoop-env.sh 文件</span></span><br><span class="line">$ vim hadoop-env.sh</span><br><span class="line"><span class="comment">#文件最后添加 </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241 </span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root </span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root </span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root </span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root </span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 core-site.xml 文件  </span></span><br><span class="line">$ vim core-site.xml </span><br><span class="line"><span class="comment">#添加如下内容</span></span><br><span class="line">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://master:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 设置Hadoop本地保存数据路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 设置HDFS web UI用户身份 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 整合hive 用户代理设置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 文件系统垃圾桶保存时间 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 hdfs-site.xml 文件  </span></span><br><span class="line">$ vim hdfs-site.xml </span><br><span class="line">&lt;!-- 设置SNN进程运行机器位置信息 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;slave1:9868&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 mapred-site.xml 文件</span></span><br><span class="line">$ vim mapred-site.xml</span><br><span class="line"><span class="comment">#添加如下内容</span></span><br><span class="line">&lt;!-- 设置MR程序默认运行模式： yarn集群模式 <span class="built_in">local</span>本地模式 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- MR程序历史服务地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;!-- MR程序历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;master:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 yarn-site.xml 文件</span></span><br><span class="line">$ vim yarn-site.xml</span><br><span class="line"><span class="comment">#添加如下内容</span></span><br><span class="line">&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 是否将对容器实施物理内存限制 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 开启日志聚集 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置yarn历史服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 历史日志保存的时间 7天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 workers 文件</span></span><br><span class="line">$ vim workers</span><br><span class="line"><span class="comment">#将 workers 里的 localhost 删除，添加如下内容</span></span><br><span class="line">master </span><br><span class="line">slave1 </span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="（3）分发-1"><a href="#（3）分发-1" class="headerlink" title="（3）分发"></a>（3）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点将 hadoop 传输到 slave1 和 slave2</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r hadoop-3.3.0 root@slave1:<span class="variable">$PWD</span></span><br><span class="line">$ scp -r hadoop-3.3.0 root@slave2:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将 hadoop 添加到环境变量</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="comment">#在文件内添加如下内容</span></span><br><span class="line"><span class="comment"># hadoop 环境变量 </span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）</span></span><br><span class="line"><span class="comment">#配置完成后，在 master slave1 和slave2 三台主机创建软连接</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server </span><br><span class="line">$ <span class="built_in">ln</span> -s hadoop-3.3.0/ hadoop</span><br><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment">#在 master 主节点进行 Hadoop 集群启动 格式化 namenode（只有首次启动需要格式化）</span></span><br><span class="line">$ hdfs namenode -format</span><br><span class="line"><span class="comment">#等待初始化完成后，使用脚本一键起动</span></span><br><span class="line">$ start-all.sh </span><br><span class="line"><span class="comment">#起动后，输入jps查看进程号</span></span><br><span class="line">$ jps</span><br><span class="line"><span class="comment">#进程查看完毕后可进入到 WEB 界面</span></span><br><span class="line"><span class="comment">#HDFS集群的界面网站是:http://master:9870/</span></span><br><span class="line"><span class="comment">#YARN集群的界面网站是:http://master:9870/</span></span><br></pre></td></tr></table></figure><hr><h2 id="4-安装zookeeper"><a href="#4-安装zookeeper" class="headerlink" title="4.安装zookeeper"></a>4.安装zookeeper</h2><h3 id="（1）下载安装包-1"><a href="#（1）下载安装包-1" class="headerlink" title="（1）下载安装包"></a>（1）下载安装包</h3><p>本文使用的 zookeeper 是3.7.0版本<br><a href="https://zookeeper.apache.org/releases.html#download">zookeeper3.7.0安装包下载</a><br><strong>注意：下载的是后缀为 .tar.gz 的包,安装包需要3.7版本网上，否者后续Kafka配置会出现问题</strong></p><h3 id="（2）在主机-master-上安装-zookeeper"><a href="#（2）在主机-master-上安装-zookeeper" class="headerlink" title="（2）在主机 master 上安装 zookeeper"></a>（2）在主机 master 上安装 zookeeper</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上传本地下载好的 apache-zookeeper-3.7.0-bin.tar.gz 上传到 /export/server 并解压文件</span></span><br><span class="line">$ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz</span><br><span class="line"><span class="comment">#修改配置文件,进入到 /export/server 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/</span><br><span class="line"><span class="comment">#在 /export/server 目录下创建 zookeeper 软连接</span></span><br><span class="line">$ <span class="built_in">ln</span> -s apache-zookeeper-3.7.0-bin/ zookeeper</span><br><span class="line"><span class="comment">#进入到 zookeeper 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> zookeeper</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入到 zookeeper 下的 conf 文件内</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/conf/ </span><br><span class="line"><span class="comment">#将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</span></span><br><span class="line">$ <span class="built_in">cp</span> zoo_sample.cfg zoo.cfg</span><br><span class="line"><span class="comment">#在 zoo.cfg 文件内添加如下内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Zookeeper的数据存放目录</span></span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"><span class="comment"># 保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="comment"># 日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="comment"># 集群中服务器地址</span></span><br><span class="line">server.1=master:2888:3888 </span><br><span class="line">server.2=slave1:2888:3888 </span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件,将 1 写入进去</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/zkdata</span><br><span class="line">$ <span class="built_in">mkdir</span> myid</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;1&#x27;</span> &gt; myid</span><br><span class="line"><span class="comment">#查看是否成功写入</span></span><br><span class="line">$ vim myid</span><br><span class="line"><span class="comment">#出现数字1即为成功</span></span><br></pre></td></tr></table></figure><h3 id="（3）分发-2"><a href="#（3）分发-2" class="headerlink" title="（3）分发"></a>（3）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点将 zookeeper 传输到 slave1 和 slave2</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r /export/server/zookeeper/ slave1:<span class="variable">$PWD</span></span><br><span class="line">$ scp -r /export/server/zookeeper/ slave2:<span class="variable">$PWD</span></span><br><span class="line"><span class="comment">#推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和 3</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave1 节点上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/zkdatas/</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;2&#x27;</span> &gt; myid</span><br><span class="line"><span class="comment">#查看是否成功写入</span></span><br><span class="line">$ vim myid</span><br><span class="line"><span class="comment">#出现数字2即为成功</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave2 节点上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/zkdatas/</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;3&#x27;</span> &gt; myid</span><br><span class="line"><span class="comment">#查看是否成功写入</span></span><br><span class="line">$ vim myid</span><br><span class="line"><span class="comment">#出现数字3即为成功</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将 zookeeper 添加到环境变量</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="comment">#在文件内添加如下内容</span></span><br><span class="line"><span class="comment"># zookeeper 环境变量 </span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/export/server/zookeeper </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）</span></span><br><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment">#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/bin</span><br><span class="line">$ zkServer.sh start</span><br><span class="line"><span class="comment">#查看 zookeeper 的状态</span></span><br><span class="line">$ zkServer.sh status</span><br><span class="line"><span class="comment">#也可以通过jps查看zookeeper的进程</span></span><br><span class="line">$ jps</span><br></pre></td></tr></table></figure><h2 id="以上-就是Kafka基础环境的配置-接下来会带来-Kafka命令行操作"><a href="#以上-就是Kafka基础环境的配置-接下来会带来-Kafka命令行操作" class="headerlink" title="以上,就是Kafka基础环境的配置,接下来会带来 Kafka命令行操作"></a>以上,就是Kafka基础环境的配置,接下来会带来 Kafka命令行操作</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文主要使用Hexo与Github进行个人blog的搭建&lt;br&gt;Hexo官网：&lt;a href=&quot;https://hexo.io/zh-cn/&quot;&gt;Hexo&lt;/a&gt;&lt;br&gt;Github官网：&lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;</summary>
      
    
    
    
    <category term="工具" scheme="http://example.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
    <category term="主题" scheme="http://example.com/tags/%E4%B8%BB%E9%A2%98/"/>
    
    <category term="搭建" scheme="http://example.com/tags/%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Spark HA &amp; Yarn配置</title>
    <link href="http://example.com/2022/05/19/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/19/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-19T08:19:25.354Z</published>
    <updated>2022-05-19T10:44:31.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本文着重描述-Spark-HA-模式和-Spark-on-Yarn-模式"><a href="#本文着重描述-Spark-HA-模式和-Spark-on-Yarn-模式" class="headerlink" title="本文着重描述 Spark HA 模式和 Spark on Yarn 模式"></a>本文着重描述 Spark HA 模式和 Spark on Yarn 模式</h2><p><strong>注意：以下操作是基于Spark local &amp; stand-alone配置,若未操作,请移步到[Spark local &amp; stand-alone配置](..&#x2F;..&#x2F;17&#x2F;Spark local &amp; stand-alone配置)进行配置</strong></p><h2 id="1-Spark-HA-模式"><a href="#1-Spark-HA-模式" class="headerlink" title="1.Spark HA 模式"></a>1.Spark HA 模式</h2><p>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题；aster故障后，集群就不可用。在HA模式下当Active的Master出现故障时,另外的一个Standby Master会被选举出来</p><h3 id="（1）更改-Spark-配置文件内容"><a href="#（1）更改-Spark-配置文件内容" class="headerlink" title="（1）更改 Spark 配置文件内容"></a>（1）更改 Spark 配置文件内容</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在虚拟机 master 上进入到/export/server/spark/conf/目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/conf/</span><br><span class="line"><span class="comment">#向conf文件下的 spark-env.sh 文件添加内容</span></span><br></pre></td></tr></table></figure><p>**在进行添加内容之前,要删除或者注释掉 export SPARK_MASTER_HOST&#x3D;master **<br><strong>代码位于文件83行,要显示行数可通过:set nu查看</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ vim spark-env.sh</span><br><span class="line"><span class="comment">#在文件末添加如下内容</span></span><br><span class="line">SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER - Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span> </span><br><span class="line"><span class="comment"># spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 </span></span><br><span class="line"><span class="comment"># 指定Zookeeper的连接地址 </span></span><br><span class="line"><span class="comment"># 指定在Zookeeper中注册临时节点的路径</span></span><br></pre></td></tr></table></figure><h3 id="（2）分发"><a href="#（2）分发" class="headerlink" title="（2）分发"></a>（2）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将更改的文件到slave1、slave2上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/conf/</span><br><span class="line">$ scp spark-env.sh slave1:/export/server/spark/conf/</span><br><span class="line">$ scp spark-env.sh slave2:/export/server/spark/conf/</span><br></pre></td></tr></table></figure><h3 id="（3）开启"><a href="#（3）开启" class="headerlink" title="（3）开启"></a>（3）开启</h3><p>三台主机启动Zookeeper</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/bin</span><br><span class="line">$ zkServer.sh start</span><br><span class="line"><span class="comment">#查看 zookeeper 的状态</span></span><br><span class="line">$ zkServer.sh status</span><br><span class="line"><span class="comment">#也可以通过jps查看zookeeper的进程</span></span><br><span class="line">$ jps</span><br></pre></td></tr></table></figure><p>master启动hadoop</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 master 主节点启动hadoop</span></span><br><span class="line"><span class="comment">#使用脚本一键起动</span></span><br><span class="line">$ start-all.sh </span><br><span class="line"><span class="comment">#起动后，输入jps查看进程号</span></span><br><span class="line">$ jps</span><br></pre></td></tr></table></figure><p>master主节点启动master和worker进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 master 主节点进入到 /export/server/spark/sbin/ 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/sbin/ </span><br><span class="line"><span class="comment">#开启master和worker进程</span></span><br><span class="line">$ sh start-all.sh</span><br></pre></td></tr></table></figure><p><img src="/../images/master_Master.png" alt="master_Master"><br><img src="/../images/slave1_Worker.png" alt="slave1_Worker"><br><img src="/../images/slave2_Worker.png" alt="slave2_Worker"><br>访问 WebUI 界面<a href="http://master:8080/">http://master:8080/</a><br><strong>出现访问不成功的时候可能是端口被占用，将8080换为8081、8082….进行尝试</strong></p><p>访问WebUI界面，查看slave1和slave2的状态，可以看到slave1的状态为ALIVE，slave2状态为STANDBY</p><p><img src="/../images/master_alive.jpg" alt="master_alive"><br><img src="/../images/slave1_standby.jpg" alt="slave1_standby"></p><p>此时 kill 掉 master 上的 master 的 16551 进程号，通过jps查看端口是否删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kill 进程16551</span></span><br><span class="line">$ <span class="built_in">kill</span> -9 16551</span><br><span class="line">$ jps</span><br></pre></td></tr></table></figure><p><strong>注意:删除的进程号,需要是主机master通过jps查看后的Master进程号</strong><br><img src="/../images/master_kill.png" alt="master_kill"><br>刷新node2WebUI界面，发现其状态由STANDBY变为ALIVE<br><img src="/../images/slave1_alive.jpg" alt="slave1_alive"><br><strong>此时master不能在访问,需要更改端口号才能访问,逐次增加1进行尝试</strong></p><hr><h2 id="2-Spark-on-Yarn-模式"><a href="#2-Spark-on-Yarn-模式" class="headerlink" title="2.Spark on Yarn 模式"></a>2.Spark on Yarn 模式</h2><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端 </p><p>Spark在YARN上的角色</p><ul><li>Master角色由YARN的ResourceManager担任</li><li>Worker角色由YARN的NodeManager担任.</li><li>Driver角色运行在YARN容器内或提交任务的客户端进程中</li><li>Executor运行在YARN提供的容器内</li></ul><h3 id="（1）更改-Spark-配置文件内容-1"><a href="#（1）更改-Spark-配置文件内容-1" class="headerlink" title="（1）更改 Spark 配置文件内容"></a>（1）更改 Spark 配置文件内容</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在虚拟机 master 上进入到/export/server/spark/conf/目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/conf/</span><br><span class="line"><span class="comment">#向conf文件下的 spark-env.sh 文件添加内容</span></span><br><span class="line">$ vim spark-env.sh</span><br><span class="line"><span class="comment">#在文件末添加如下内容</span></span><br><span class="line"><span class="comment">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 </span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop </span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="（2）运行"><a href="#（2）运行" class="headerlink" title="（2）运行"></a>（2）运行</h3><p>在YARN上运行spark</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可以直接运行以下命令</span></span><br><span class="line">$ /export/server/spark/bin/pyspark --master yarn</span><br><span class="line"><span class="comment">#出现Spark表示,表明可以在YARN集群上运行spark</span></span><br></pre></td></tr></table></figure><p>client模式测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入到 /export/server/spark/</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/</span><br><span class="line"><span class="comment">#进行 client 测试</span></span><br><span class="line">$ /bin/spark-submit --master yarn --deploy-mode client -- driver-memory 512m --executor-memory 512m --num-executors 1 --total- executor-cores 2 <span class="variable">$&#123;SPARK_HOME&#125;</span>/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure><p><img src="/../images/master_client.jpg" alt="master_client"><br>cluster模式测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入到 /export/server/spark/</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/</span><br><span class="line"><span class="comment">#进行 cluster 测试</span></span><br><span class="line">$ /bin/spark-submit --master yarn --deploy-mode cluster -- driver-memory 512m --executor-memory 512m --num-executors 1 --total- executor-cores 2 <span class="variable">$&#123;SPARK_HOME&#125;</span>/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure><p><img src="/../images/master_cluster.jpg" alt="master_cluster"></p><p><strong>若要查看它的具体运行情况，需要开启hadoop的历史服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入到 /export/server/hadoop/sbin</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/hadoop/sbin</span><br><span class="line"><span class="comment">#启动 hadoop 历史服务器</span></span><br><span class="line">$ ./mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>启动历史服务器后,访问WebUI界面，查看client模式下的运行状况<br><img src="/../images/client.jpg" alt="client"></p><p>启动历史服务器后,访问WebUI界面，查看cluster模式下的运行状况<br><img src="/../images/cluster.jpg" alt="cluster"></p><h2 id="以上-就是-Spark-HA-amp-Yarn配置"><a href="#以上-就是-Spark-HA-amp-Yarn配置" class="headerlink" title="以上,就是 Spark HA &amp; Yarn配置"></a>以上,就是 Spark HA &amp; Yarn配置</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;本文着重描述-Spark-HA-模式和-Spark-on-Yarn-模式&quot;&gt;&lt;a href=&quot;#本文着重描述-Spark-HA-模式和-Spark-on-Yarn-模式&quot; class=&quot;headerlink&quot; title=&quot;本文着重描述 Spark HA 模式和 </summary>
      
    
    
    
    <category term="工具" scheme="http://example.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
    <category term="主题" scheme="http://example.com/tags/%E4%B8%BB%E9%A2%98/"/>
    
    <category term="搭建" scheme="http://example.com/tags/%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Spark local &amp; stand-alone配置</title>
    <link href="http://example.com/2022/05/17/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/17/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-17T12:18:47.360Z</published>
    <updated>2022-06-15T12:49:23.256Z</updated>
    
    <content type="html"><![CDATA[<p>Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群) </p><ul><li>Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试</li><li>Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在linux系统之上</li><li>hadoop YARN 模式:Spark中的各个角色运行在YARN的容器内部,并组成Spark集群环境,运行在yarn容器内</li><li>Spark中的各个角色运行在Kubernetes的容器内部,并组成Spark集群环境</li></ul><hr><h2 id="本文着重描述-Spark-local-模式和-Spark-alone-模式"><a href="#本文着重描述-Spark-local-模式和-Spark-alone-模式" class="headerlink" title="本文着重描述 Spark local 模式和 Spark alone 模式"></a><strong>本文着重描述 Spark local 模式和 Spark alone 模式</strong></h2><h2 id="1-Spark-local-模式"><a href="#1-Spark-local-模式" class="headerlink" title="1.Spark local 模式"></a>1.Spark local 模式</h2><p><strong>Spark local 模式是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时的环境</strong><br>Spark由四类角色组成整个Spark的运行环境:</p><ul><li>Master角色，管理整个集群的资源</li><li>Worker角色，管理单个服务器的资源</li><li>Driver角色，管理单个Spark任务在运行的时候的工作</li><li>Executor角色，单个任务运行的时候的工作者</li></ul><p><strong>注意：以下操作需要完成 Spark 基础环境配置。具体配置移步到<a href="../../17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">Spark基础环境配置</a></strong></p><h3 id="（1）Anaconda安装"><a href="#（1）Anaconda安装" class="headerlink" title="（1）Anaconda安装"></a>（1）Anaconda安装</h3><p>Anaconda安装包下载，不建议去官网下载，官网下载太慢了，推荐下载地址<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">Anaconda清华镜像站</a><br><strong>注意：下载的是 Anaconda3-2021.05-Linux-x86_64.sh 后缀为 .sh 的安装包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上传本地下载好的 Anaconda 安装包上到 /export/server/ 目录下进行安装</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line"><span class="comment">#执行文件</span></span><br><span class="line">$ sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p><strong>1)在遇到 Do you accept the license terms? [yes|no]时，选择yes</strong><br><img src="/../images/yes.png"><br><strong>2)在上述命令回车后，会让你选择你想要安装的路径，统一安装在&#x2F;export&#x2F;server&#x2F;anaconda3下</strong><br><img src="/../images/2.png"><br><strong>3)等待执行完毕后，在新的[yes|no]选择界面选择yes；随后exit退出</strong><br><strong>重新登录即可看到base，代表着安装完成</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建虚拟环境pyspark，基于Python 3.8</span></span><br><span class="line">$ conda create -n pyspark python=3.8</span><br><span class="line"><span class="comment">#切换到虚拟环境内</span></span><br><span class="line">$ conda activate pyspark</span><br></pre></td></tr></table></figure><p><strong>4)看到(pyspark)表示成功</strong><br><img src="/../images/3.jpg"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在虚拟环境内安装包</span></span><br><span class="line">$ pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><h3 id="（2）Spark安装"><a href="#（2）Spark安装" class="headerlink" title="（2）Spark安装"></a>（2）Spark安装</h3><p>本文使用的 spark 是3.2.0版本<br><a href="https://archive.apache.org/dist/spark/spark-3.2.0/">spark 3.2.0 安装包下载</a><br><strong>注意：下载要注意hadoop的版本，同时选择后缀为 .tar.gz的安装包</strong><br><strong>本文使用的安装包是spark-3.2.0-bin-hadoop3.2.tgz</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把本地下载好的spark-3.2.0-bin-hadoop3.2.tgz安装包上传到 /export/server 并解压</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/</span><br><span class="line">$ tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br><span class="line"><span class="comment">#建立软连接</span></span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br><span class="line"><span class="comment">#编辑环境变量</span></span><br><span class="line">$ vim /etc/profile</span><br><span class="line"><span class="comment">#在文件末添加以下内容</span></span><br><span class="line"><span class="comment">#SPARK_HOME </span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/server/spark </span><br><span class="line"><span class="comment">#HADOOP_CONF_DIR </span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop </span><br><span class="line"><span class="comment">#PYSPARK_PYTHON </span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编辑.bashrc文件，添加java和pyspark的Home值</span></span><br><span class="line">$ vim .bashrc</span><br><span class="line"><span class="comment">#添加以下内容</span></span><br><span class="line"><span class="comment">#JAVA_HOME </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241 </span><br><span class="line"><span class="comment">#PYSPARK_PYTHON </span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="comment">#进入 /export/server/anaconda3/envs/pyspark/bin/ 文件夹</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/anaconda3/envs/pyspark/bin/</span><br><span class="line"><span class="comment">#开启 Spark</span></span><br><span class="line">$ ./pyspark</span><br></pre></td></tr></table></figure><p><strong>看到 pyspark 表示成功</strong><br><img src="/../images/spark.jpg" alt="spark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试运行基于python的spark解释器环境，在下方运行以下代码</span></span><br><span class="line">$ sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()</span><br><span class="line"><span class="comment">#结果出来后,查看WebUI界面:http://master:4040/</span></span><br></pre></td></tr></table></figure><p><img src="/../images/map.jpg" alt="map"></p><h2 id="2-Spark-alone-模式"><a href="#2-Spark-alone-模式" class="headerlink" title="2.Spark alone 模式"></a>2.Spark alone 模式</h2><p><strong>tand-alone集群模式中，Spark的各个角色以独立进程的形式存在，并组成Spark集群环境</strong><br>StandAlone集群在进程上主要有三类:</p><ul><li>主节点Master进程：Master角色，管理整个集群资源，并托管各个任务的Driver</li><li>从节点Workers：Worker角色，管理每个机器的资源，分配对应资源来运行Executor（Task）</li><li>历史服务器HistoryServer：在Spark Application运行完成以后，保存事件日志数据至HDFS</li></ul><h3 id="（1）Spark-alone-模式下的新配置"><a href="#（1）Spark-alone-模式下的新配置" class="headerlink" title="（1）Spark alone 模式下的新配置"></a>（1）Spark alone 模式下的新配置</h3><p><strong>前提:三台虚拟机全部安装 Anaconda</strong><br><strong>参考Spark（local）模式下的Anaconda的安装文档，在slave1、slave2完成对Anaconda的安装</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点节点进入 /export/server/spark/conf 修改以下配置文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/conf</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将文件 workers.template 改名为 workers，并配置文件内容</span></span><br><span class="line">$ <span class="built_in">mv</span> workers.template workers</span><br><span class="line"><span class="comment">#修改 workers 文件</span></span><br><span class="line">$ vim workers</span><br><span class="line"><span class="comment">#将 workers 里的 localhost 删除，添加如下内容</span></span><br><span class="line">master </span><br><span class="line">slave1 </span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</span></span><br><span class="line">$ <span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment">#修改 spark-env.sh 文件</span></span><br><span class="line">$ vim spark-env.sh</span><br><span class="line"><span class="comment">#文件最后添加以下内容</span></span><br><span class="line"><span class="comment">## 设置JAVA安装目录 </span></span><br><span class="line">JAVA_HOME=/export/server/jdk </span><br><span class="line"><span class="comment">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 </span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop </span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop </span><br><span class="line"><span class="comment">## 指定spark老大Master的IP和提交任务的通信端口 </span></span><br><span class="line"><span class="comment"># 告知Spark的master运行在哪个机器上 </span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=master </span><br><span class="line"><span class="comment"># 告知sparkmaster的通讯端口 </span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077 </span><br><span class="line"><span class="comment"># 告知spark master的 webui端口 </span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080 </span><br><span class="line"><span class="comment"># worker cpu可用核数 </span></span><br><span class="line">SPARK_WORKER_CORES=1 </span><br><span class="line"><span class="comment"># worker可用内存 </span></span><br><span class="line">SPARK_WORKER_MEMORY=1g </span><br><span class="line"><span class="comment"># worker的工作通讯地址 </span></span><br><span class="line">SPARK_WORKER_PORT=7078 </span><br><span class="line"><span class="comment"># worker的 webui地址 </span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081 </span><br><span class="line"><span class="comment">## 设置历史服务器 </span></span><br><span class="line"><span class="comment"># 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 </span></span><br><span class="line">SPARK_HISTORY_OPTS=<span class="string">&quot;- Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - Dspark.history.fs.cleaner.enabled=true&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#开启hadoop服务</span></span><br><span class="line">$ start-all.sh</span><br><span class="line"><span class="comment">#在HDFS上创建程序运行历史记录存放文件夹</span></span><br><span class="line">$ hadoop fs -<span class="built_in">mkdir</span> /sparklog</span><br><span class="line"><span class="comment">#已存在会显示File exists,属于正常状况</span></span><br><span class="line"><span class="comment">#给sparklog添加权限</span></span><br><span class="line">$ hadoop fs -<span class="built_in">chmod</span> 777 /sparklog</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新进入到 /export/server/spark/conf/ 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/conf/</span><br><span class="line"><span class="comment">#将spark-defaults.conf.template 文件改为spark-defaults.conf</span></span><br><span class="line">$ <span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="comment">#编辑 spark-defaults.conf</span></span><br><span class="line">$ vim spark-defaults.conf</span><br><span class="line"><span class="comment">#文件最后添加以下内容</span></span><br><span class="line"><span class="comment"># 开启spark的日期记录功能 </span></span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span> </span><br><span class="line"><span class="comment"># 设置spark日志记录的路径 </span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span> hdfs://master:8020/sparklog/ </span><br><span class="line"><span class="comment"># 设置spark日志是否启动压缩 </span></span><br><span class="line">spark.eventLog.compress <span class="literal">true</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将log4j.properties.template文件改为log4j.properties</span></span><br><span class="line">$ <span class="built_in">mv</span> log4j.properties.template log4j.properties</span><br></pre></td></tr></table></figure><p><strong>配置 log4j.properties 文件</strong><br><strong>将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为log4j.rootCategory&#x3D;WARN, console</strong><br><img src="/../images/properties.jpg" alt="properties"></p><h3 id="（2）分发"><a href="#（2）分发" class="headerlink" title="（2）分发"></a>（2）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</span></span><br><span class="line">$ <span class="built_in">cd</span> /```bash</span><br><span class="line"><span class="comment">#在 slave1 节点上</span></span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br><span class="line"><span class="comment">#重新加载环境变量</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave2 节点上</span></span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br><span class="line"><span class="comment">#重新加载环境变量</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">```<span class="built_in">export</span>/server</span><br><span class="line">$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave1:<span class="variable">$PWD</span></span><br><span class="line">$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave2:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><p><strong>在slave1 和 slave2 上做软连接</strong></p><p><strong>完成上述操作后，启动历史服务器</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 master 主机上进入 /export/server/spark/sbin 文件目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/sbin</span><br><span class="line"><span class="comment">#启动历史服务器</span></span><br><span class="line">$ ./start-history-server.sh</span><br><span class="line"><span class="comment">#访问 WebUI 界面:http://master:18080/</span></span><br></pre></td></tr></table></figure><p><img src="/../images/history.jpg" alt="history"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动Spark的Master和Worker进程</span></span><br><span class="line">$ sh start-all.sh</span><br><span class="line"><span class="comment">#通过jps查看进程是否开启master和worker进程</span></span><br><span class="line">$ jps </span><br><span class="line">在查看到 Master 和 Worker 进程后表明成功启动</span><br><span class="line"><span class="comment">#访问 WebUI界面:http://master:8080/</span></span><br></pre></td></tr></table></figure><p><img src="/../images/worker.jpg" alt="worker"></p><h2 id="以上-就是-Spark-local-amp-stand-alone配置-接下来会带来-Spark-HA-amp-Yarn配置"><a href="#以上-就是-Spark-local-amp-stand-alone配置-接下来会带来-Spark-HA-amp-Yarn配置" class="headerlink" title="以上,就是 Spark local&amp; stand-alone配置,接下来会带来 Spark HA &amp; Yarn配置"></a>以上,就是 Spark local&amp; stand-alone配置,接下来会带来 Spark HA &amp; Yarn配置</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群) &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark local 模式:以一个独立的进程,通过其内</summary>
      
    
    
    
    <category term="工具" scheme="http://example.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
    <category term="主题" scheme="http://example.com/tags/%E4%B8%BB%E9%A2%98/"/>
    
    <category term="搭建" scheme="http://example.com/tags/%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Spark基础环境配置</title>
    <link href="http://example.com/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-17T12:18:46.617Z</published>
    <updated>2022-06-15T09:39:40.334Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要使用Hexo与Github进行个人blog的搭建<br>Hexo官网：<a href="https://hexo.io/zh-cn/">Hexo</a><br>Github官网：<a href="https://github.com/">Github</a></p><h3 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h3><p>本地环境为: Window10系统、Linux虚拟机<br><strong>注意：本文配置与Kafka基础环境配置相同，若Kafka基础环境配置已配置，请直接观看下一文章</strong></p><hr><h1 id="开始搭建"><a href="#开始搭建" class="headerlink" title="开始搭建"></a>开始搭建</h1><h2 id="1-基础环境"><a href="#1-基础环境" class="headerlink" title="1.基础环境"></a>1.基础环境</h2><p>在开始配置前，需要检查虚拟机主机名、hosts映射、关闭防火墙、免密登录、同步时间等操作</p><h3 id="（1）编辑主机名（三台机器）"><a href="#（1）编辑主机名（三台机器）" class="headerlink" title="（1）编辑主机名（三台机器）"></a>（1）编辑主机名（三台机器）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看系统主机名(三台主机)</span></span><br><span class="line">$ <span class="built_in">cat</span> /etc/hostname</span><br><span class="line"><span class="comment">#在三台主机上更改主机名</span></span><br><span class="line"><span class="comment">#在 master 主节点</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;master&quot;</span> &gt;/etc/hostname </span><br><span class="line"><span class="comment">#在 slave1 节点 </span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;slave1&quot;</span> &gt;/etc/hostname </span><br><span class="line"><span class="comment">#在 slave2 节点</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;slave2&quot;</span> &gt;/etc/hostname</span><br></pre></td></tr></table></figure><h3 id="（2）hosts映射"><a href="#（2）hosts映射" class="headerlink" title="（2）hosts映射"></a>（2）hosts映射</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看系统映射</span></span><br><span class="line">$ <span class="built_in">cat</span> /etc/hosts</span><br><span class="line"><span class="comment">#编辑 /etc/hosts 文件</span></span><br><span class="line">$ vim /etc/hosts</span><br><span class="line"><span class="comment">#内容修改为 （三台主机内容一致）</span></span><br><span class="line">127.0.0.1 localhost localhost.localdomain localhost4 </span><br><span class="line">localhost4.localdomain4 </span><br><span class="line">::1 localhost localhost.localdomain localhost6 </span><br><span class="line">localhost6.localdomain6 </span><br><span class="line"></span><br><span class="line">192.168.88.135 master </span><br><span class="line">192.168.88.136 slave1 </span><br><span class="line">192.168.88.137 slave2</span><br></pre></td></tr></table></figure><h3 id="（3）关闭防火墙"><a href="#（3）关闭防火墙" class="headerlink" title="（3）关闭防火墙"></a>（3）关闭防火墙</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#关闭防火墙</span></span><br><span class="line">$ systemctl stop firewalld.service</span><br><span class="line"><span class="comment">#禁止防火墙开启自启</span></span><br><span class="line">$ systemctl <span class="built_in">disable</span> firewalld.service</span><br></pre></td></tr></table></figure><h3 id="（4）免密登录"><a href="#（4）免密登录" class="headerlink" title="（4）免密登录"></a>（4）免密登录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 生成公钥私钥，四个回车即可</span></span><br><span class="line">$ ssh-keygen</span><br><span class="line"><span class="comment">#master 配置免密登录到master slave1 slave2三台主机</span></span><br><span class="line">$ ssh-copy-id master </span><br><span class="line">$ ssh-copy-id slave1 </span><br><span class="line">$ ssh-copy-id slave2</span><br></pre></td></tr></table></figure><h3 id="（5）时间同步"><a href="#（5）时间同步" class="headerlink" title="（5）时间同步"></a>（5）时间同步</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装 ntp</span></span><br><span class="line">$ yum install ntp -y </span><br><span class="line"><span class="comment">#设置 ntp 开机自启动</span></span><br><span class="line">$ systemctl <span class="built_in">enable</span> ntpd &amp;&amp; systemctl start ntpd</span><br><span class="line"><span class="comment">#三台主机分别运行以下命令</span></span><br><span class="line">$ ntpdate ntp4.aliyun.com</span><br></pre></td></tr></table></figure><hr><h2 id="2-JDK安装"><a href="#2-JDK安装" class="headerlink" title="2.JDK安装"></a>2.JDK安装</h2><h3 id="（1）下载安装包"><a href="#（1）下载安装包" class="headerlink" title="（1）下载安装包"></a>（1）下载安装包</h3><p>本文使用的 JDK 是1.8版本<br><a href="https://www.oracle.com/java/technologies/downloads/#java8">jdk1.8安装包下载</a><br><strong>注意：下载的是后缀为 .tar.gz 的包</strong></p><h3 id="（2）在主机-master-上安装-JDK"><a href="#（2）在主机-master-上安装-JDK" class="headerlink" title="（2）在主机 master 上安装 JDK"></a>（2）在主机 master 上安装 JDK</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译环境软件安装目录</span></span><br><span class="line">$ <span class="built_in">mkdir</span> -p /export/server</span><br><span class="line"><span class="comment">#上传本地下载好的jdk-8u241-linux-x64.tar.gz上传到/export/server/目录下 并解压文件</span></span><br><span class="line">$ tar -zxvf jdk-8u241-linux-x64.tar.gz</span><br><span class="line"><span class="comment">#配置环境变量</span></span><br><span class="line">$ vim /etc/profile</span><br><span class="line"><span class="comment">#在文件内添加如下内容</span></span><br><span class="line"><span class="comment"># jdk 环境变量 </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241 </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin </span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.ja</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment">#查看 java 版本号</span></span><br><span class="line">$ java -version</span><br><span class="line"><span class="comment">#出现 java version &quot;1.8.0_241&quot; 表示安装成功</span></span><br></pre></td></tr></table></figure><h3 id="（3）分发"><a href="#（3）分发" class="headerlink" title="（3）分发"></a>（3）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点将 java 传输到 slave1 和 slave2</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ </span><br><span class="line">$ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/</span><br><span class="line"><span class="comment">#配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）</span></span><br><span class="line"><span class="comment">#配置完成后，在 master slave1 和slave2 三台主机创建软连接</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server </span><br><span class="line">$ <span class="built_in">ln</span> -s jdk1.8.0_241/ jdk</span><br><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><hr><h2 id="3-Hadoop安装"><a href="#3-Hadoop安装" class="headerlink" title="3.Hadoop安装"></a>3.Hadoop安装</h2><h3 id="（1）下载安装包-1"><a href="#（1）下载安装包-1" class="headerlink" title="（1）下载安装包"></a>（1）下载安装包</h3><p>本文使用的 hadoop 是3.3.0版本<br><a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz">hadoop3.3.0安装包下载</a><br><strong>注意：下载的是后缀为 .tar.gz的包</strong></p><h3 id="（2）在主机-master-上安装-hadoop"><a href="#（2）在主机-master-上安装-hadoop" class="headerlink" title="（2）在主机 master 上安装 hadoop"></a>（2）在主机 master 上安装 hadoop</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上传本地下载好的 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件</span></span><br><span class="line">$ tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</span><br><span class="line"><span class="comment">#修改配置文件,进入到 hadoop 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/hadoop-3.3.0/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment">#编辑 hadoop-env.sh 文件</span></span><br><span class="line">$ vim hadoop-env.sh</span><br><span class="line"><span class="comment">#文件最后添加 </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241 </span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root </span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root </span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root </span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root </span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 core-site.xml 文件  </span></span><br><span class="line">$ vim core-site.xml </span><br><span class="line"><span class="comment">#添加如下内容</span></span><br><span class="line">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 -</span><br><span class="line">-&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置Hadoop本地保存数据路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置HDFS web UI用户身份 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 整合hive 用户代理设置 --&gt;</span><br><span class="line">hdfs-site.xml</span><br><span class="line">mapred-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 文件系统垃圾桶保存时间 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 mapred-site.xml 文件</span></span><br><span class="line">$ vim mapred-site.xml</span><br><span class="line"><span class="comment">#添加如下内容</span></span><br><span class="line">&lt;!-- 设置MR程序默认运行模式： yarn 集群模式 <span class="built_in">local</span>本地模式 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- MR程序历史服务地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- MR程序历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;master:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 hdfs-site.xml 文件</span></span><br><span class="line">$ vim hdfs-site.xml</span><br><span class="line"><span class="comment">#添加如下内容</span></span><br><span class="line">&lt;!-- 设置SNN进程运行机器位置信息 --&gt; </span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; </span><br><span class="line">  &lt;value&gt;slave1:9868&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 yarn-site.xml 文件</span></span><br><span class="line">$ vim yarn-site.xml</span><br><span class="line"><span class="comment">#添加如下内容</span></span><br><span class="line">&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 是否将对容器实施物理内存限制 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 开启日志聚集 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置yarn历史服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 历史日志保存的时间 7天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 workers 文件</span></span><br><span class="line">$ vim workers</span><br><span class="line"><span class="comment">#将 workers 里的 localhost 删除，添加如下内容</span></span><br><span class="line">master </span><br><span class="line">slave1 </span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="（3）分发-1"><a href="#（3）分发-1" class="headerlink" title="（3）分发"></a>（3）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点将 hadoop 传输到 slave1 和 slave2</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r hadoop-3.3.0 root@slave1:<span class="variable">$PWD</span></span><br><span class="line">$ scp -r hadoop-3.3.0 root@slave2:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将 hadoop 添加到环境变量</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="comment">#在文件内添加如下内容</span></span><br><span class="line"><span class="comment"># hadoop 环境变量 </span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）</span></span><br><span class="line"><span class="comment">#配置完成后，在 master slave1 和slave2 三台主机创建软连接</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server </span><br><span class="line">$ <span class="built_in">ln</span> -s hadoop-3.3.0/ hadoop</span><br><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment">#在 master 主节点进行 Hadoop 集群启动 格式化 namenode（只有首次启动需要格式化）</span></span><br><span class="line">$ hdfs namenode -format</span><br><span class="line"><span class="comment">#等待初始化完成后，使用脚本一键起动</span></span><br><span class="line">$ start-all.sh </span><br><span class="line"><span class="comment">#起动后，输入jps查看进程号</span></span><br><span class="line">$ jps</span><br><span class="line"><span class="comment">#进程查看完毕后可进入到 WEB 界面</span></span><br><span class="line"><span class="comment">#HDFS集群的界面网站是:http://master:9870/</span></span><br><span class="line"><span class="comment">#YARN集群的界面网站是:http://master:9870/</span></span><br></pre></td></tr></table></figure><hr><h2 id="4-安装zookeeper"><a href="#4-安装zookeeper" class="headerlink" title="4.安装zookeeper"></a>4.安装zookeeper</h2><h3 id="（1）下载安装包-2"><a href="#（1）下载安装包-2" class="headerlink" title="（1）下载安装包"></a>（1）下载安装包</h3><p>本文使用的 zookeeper 是3.7.0版本<br><a href="https://zookeeper.apache.org/releases.html#download">zookeeper3.7.0安装包下载</a><br><strong>注意：下载的是后缀为 .tar.gz 的包,安装包需要3.7版本网上，否者后续spark配置会出现问题</strong></p><h3 id="（2）在主机-master-上安装-zookeeper"><a href="#（2）在主机-master-上安装-zookeeper" class="headerlink" title="（2）在主机 master 上安装 zookeeper"></a>（2）在主机 master 上安装 zookeeper</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上传本地下载好的 apache-zookeeper-3.7.0-bin.tar.gz 上传到 /export/server 并解压文件</span></span><br><span class="line">$ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz</span><br><span class="line"><span class="comment">#修改配置文件,进入到 /export/server 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/</span><br><span class="line"><span class="comment">#在 /export/server 目录下创建 zookeeper 软连接</span></span><br><span class="line">$ <span class="built_in">ln</span> -s apache-zookeeper-3.7.0-bin/ zookeeper</span><br><span class="line"><span class="comment">#进入到 zookeeper 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> zookeeper</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入到 zookeeper 下的 conf 文件内</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/conf/ </span><br><span class="line"><span class="comment">#将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</span></span><br><span class="line">$ <span class="built_in">cp</span> zoo_sample.cfg zoo.cfg</span><br><span class="line"><span class="comment">#在 zoo.cfg 文件内添加如下内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Zookeeper的数据存放目录</span></span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"><span class="comment"># 保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="comment"># 日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="comment"># 集群中服务器地址</span></span><br><span class="line">server.1=master:2888:3888 </span><br><span class="line">server.2=slave1:2888:3888 </span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件,将 1 写入进去</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/zkdata</span><br><span class="line">$ <span class="built_in">mkdir</span> myid</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;1&#x27;</span> &gt; myid</span><br><span class="line"><span class="comment">#查看是否成功写入</span></span><br><span class="line">$ vim myid</span><br><span class="line"><span class="comment">#出现数字1即为成功</span></span><br></pre></td></tr></table></figure><h3 id="（3）分发-2"><a href="#（3）分发-2" class="headerlink" title="（3）分发"></a>（3）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点将 zookeeper 传输到 slave1 和 slave2</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r /export/server/zookeeper/ slave1:<span class="variable">$PWD</span></span><br><span class="line">$ scp -r /export/server/zookeeper/ slave2:<span class="variable">$PWD</span></span><br><span class="line"><span class="comment">#推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和 3</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave1 节点上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/zkdatas/</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;2&#x27;</span> &gt; myid</span><br><span class="line"><span class="comment">#查看是否成功写入</span></span><br><span class="line">$ vim myid</span><br><span class="line"><span class="comment">#出现数字2即为成功</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave2 节点上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/zkdatas/</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;3&#x27;</span> &gt; myid</span><br><span class="line"><span class="comment">#查看是否成功写入</span></span><br><span class="line">$ vim myid</span><br><span class="line"><span class="comment">#出现数字3即为成功</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将 zookeeper 添加到环境变量</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="comment">#在文件内添加如下内容</span></span><br><span class="line"><span class="comment"># zookeeper 环境变量 </span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/export/server/zookeeper </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样）</span></span><br><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment">#三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/zookeeper/bin</span><br><span class="line">$ zkServer.sh start</span><br><span class="line"><span class="comment">#查看 zookeeper 的状态</span></span><br><span class="line">$ zkServer.sh status</span><br><span class="line"><span class="comment">#也可以通过jps查看zookeeper的进程</span></span><br><span class="line">$ jps</span><br></pre></td></tr></table></figure><h2 id="以上-就是Spark基础环境的配置-接下来会带来-Spark-local-amp-stand-alone配置"><a href="#以上-就是Spark基础环境的配置-接下来会带来-Spark-local-amp-stand-alone配置" class="headerlink" title="以上,就是Spark基础环境的配置,接下来会带来 Spark local&amp; stand-alone配置"></a>以上,就是Spark基础环境的配置,接下来会带来 Spark local&amp; stand-alone配置</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文主要使用Hexo与Github进行个人blog的搭建&lt;br&gt;Hexo官网：&lt;a href=&quot;https://hexo.io/zh-cn/&quot;&gt;Hexo&lt;/a&gt;&lt;br&gt;Github官网：&lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;</summary>
      
    
    
    
    <category term="工具" scheme="http://example.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
    <category term="主题" scheme="http://example.com/tags/%E4%B8%BB%E9%A2%98/"/>
    
    <category term="搭建" scheme="http://example.com/tags/%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
</feed>

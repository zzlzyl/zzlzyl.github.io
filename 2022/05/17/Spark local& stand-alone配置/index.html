<!DOCTYPE html>
<html lang=zh>
<head>
    <meta charset="utf-8">
    
    <title>Spark local &amp; stand-alone配置-中文文档 | zzlzyl</title>
    
    
        <meta name="keywords" content="hexo,主题,搭建" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群)   Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试 Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在l">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark local &amp; stand-alone配置-中文文档">
<meta property="og:url" content="http://example.com/2022/05/17/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/index.html">
<meta property="og:site_name" content="zzlzyl">
<meta property="og:description" content="Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群)   Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试 Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在l">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/yes.png">
<meta property="og:image" content="http://example.com/images/2.png">
<meta property="og:image" content="http://example.com/images/3.jpg">
<meta property="og:image" content="http://example.com/images/spark.jpg">
<meta property="og:image" content="http://example.com/images/map.jpg">
<meta property="og:image" content="http://example.com/images/properties.jpg">
<meta property="og:image" content="http://example.com/images/history.jpg">
<meta property="og:image" content="http://example.com/images/worker.jpg">
<meta property="article:published_time" content="2022-05-17T12:18:47.360Z">
<meta property="article:modified_time" content="2022-05-18T04:20:15.972Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="hexo">
<meta property="article:tag" content="主题">
<meta property="article:tag" content="搭建">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/yes.png">
    

    
        <link rel="alternate" href="/atom.xml" title="zzlzyl" type="application/atom+xml" />
    

    
        <link rel="icon" href="/css/images/favicon.ico" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">zzlzyl</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">首页</a>
                
                    <a class="main-nav-link" href="/archives">归档</a>
                
                    <a class="main-nav-link" href="/categories">分类</a>
                
                    <a class="main-nav-link" href="/tags">标签</a>
                
                    <a class="main-nav-link" href="/about">关于</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/css/images/au.png" />
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="想要查找什么..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/categories">分类</a></td>
                
                    <td><a class="main-nav-link" href="/tags">标签</a></td>
                
                    <td><a class="main-nav-link" href="/about">关于</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/css/images/au.png" />
            <h2 id="name">zzlzyl</h2>
            <h3 id="title">Spark &amp; Programmer</h3>
            <span id="location"><i class="fa fa-map-marker"></i>PuYang, China</span>
            <a id="follow" target="_blank" href="https://github.com/zzlzyl">关注我</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                2
                <span>文章</span>
            </div>
            <div class="article-info-block">
                3
                <span>标签</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="https://github.com/zzlzyl/zzlzyl.github.io" target="_blank" title="github" class=tooltip>
                            <i class="fa fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/atom.xml" target="_blank" title="rss" class=tooltip>
                            <i class="fa fa-rss"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>分类</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-up fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" style="display: block;"> 
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            工具
                        </a>
                         <ul class="unstyled" id="tree" style="display: block;">  <li class="file"><a href="/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">Spark基础环境配置</a></li>  <li class="file active"><a href="/2022/05/17/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/">Spark local & stand-alone配置-中文文档</a></li>  </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-Spark local&amp; stand-alone配置" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/hexo/" rel="tag">hexo</a>, <a class="tag-link-link" href="/tags/%E4%B8%BB%E9%A2%98/" rel="tag">主题</a>, <a class="tag-link-link" href="/tags/%E6%90%AD%E5%BB%BA/" rel="tag">搭建</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2022/05/17/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/">
            <time datetime="2022-05-17T12:18:47.360Z" itemprop="datePublished">2022-05-17</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/zzlzyl/blog/raw/master/source/_posts/Spark local&amp; stand-alone配置.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/zzlzyl/blog/edit/master/source/_posts/Spark local&amp; stand-alone配置.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/zzlzyl/blog/commits/master/source/_posts/Spark local&amp; stand-alone配置.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Spark local &amp; stand-alone配置-中文文档
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <p>Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群) </p>
<ul>
<li>Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试</li>
<li>Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在linux系统之上</li>
<li>hadoop YARN 模式:Spark中的各个角色运行在YARN的容器内部,并组成Spark集群环境,运行在yarn容器内</li>
<li>Spark中的各个角色运行在Kubernetes的容器内部,并组成Spark集群环境</li>
</ul>
<hr>
<h2 id="本文着重描述-Spark-local-模式和-Spark-alone-模式"><a href="#本文着重描述-Spark-local-模式和-Spark-alone-模式" class="headerlink" title="本文着重描述 Spark local 模式和 Spark alone 模式"></a><strong>本文着重描述 Spark local 模式和 Spark alone 模式</strong></h2><h2 id="1-Spark-local-模式"><a href="#1-Spark-local-模式" class="headerlink" title="1.Spark local 模式"></a>1.Spark local 模式</h2><p><strong>Spark local 模式是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时的环境</strong><br>Spark由四类角色组成整个Spark的运行环境:</p>
<ul>
<li>Master角色，管理整个集群的资源</li>
<li>Worker角色，管理单个服务器的资源</li>
<li>Driver角色，管理单个Spark任务在运行的时候的工作</li>
<li>Executor角色，单个任务运行的时候的工作者</li>
</ul>
<p><strong>注意：以下操作需要完成 Spark 基础环境配置。具体配置移步到<a href="../../17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">Spark基础环境配置</a></strong></p>
<h3 id="（1）Anaconda安装"><a href="#（1）Anaconda安装" class="headerlink" title="（1）Anaconda安装"></a>（1）Anaconda安装</h3><p>Anaconda安装包下载，不建议去官网下载，官网下载太慢了，推荐下载地址<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">Anaconda清华镜像站</a><br><strong>注意：下载的是 Anaconda3-2021.05-Linux-x86_64.sh 后缀为 .sh 的安装包</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上传本地下载好的 Anaconda 安装包上到 /export/server/ 目录下进行安装</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line"><span class="comment">#执行文件</span></span><br><span class="line">$ sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p><strong>1)在遇到 Do you accept the license terms? [yes|no]时，选择yes</strong><br><img src="/../images/yes.png" alt="yes"><br><strong>在上述命令回车后，会让你选择你想要安装的路径，统一安装在&#x2F;export&#x2F;server&#x2F;anaconda3下</strong><br><img src="/../images/2.png" alt="2"><br><strong>2)等待执行完毕后，在新的[yes|no]选择界面选择yes；随后exit退出</strong><br><strong>重新登录即可看到base，代表着安装完成</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建虚拟环境pyspark，基于Python 3.8</span></span><br><span class="line">$ conda create -n pyspark python=3.8</span><br><span class="line"><span class="comment">#切换到虚拟环境内</span></span><br><span class="line">$ conda activate pyspark</span><br></pre></td></tr></table></figure>
<p><strong>3)看到(pyspark)表示成功</strong><br><img src="/../images/3.jpg" alt="3"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在虚拟环境内安装包</span></span><br><span class="line">$ pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<h3 id="（2）Spark安装"><a href="#（2）Spark安装" class="headerlink" title="（2）Spark安装"></a>（2）Spark安装</h3><p>本文使用的 spark 是3.2.0版本<br><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/spark-3.2.0/">spark 3.2.0 安装包下载</a><br><strong>注意：下载要注意hadoop的版本，同时选择后缀为 .tar.gz的安装包</strong><br><strong>本文使用的安装包是spark-3.2.0-bin-hadoop3.2.tgz</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把本地下载好的spark-3.2.0-bin-hadoop3.2.tgz安装包上传到 /export/server 并解压</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/</span><br><span class="line">$ tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br><span class="line"><span class="comment">#建立软连接</span></span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br><span class="line"><span class="comment">#编辑环境变量</span></span><br><span class="line">$ vim /etc/profile</span><br><span class="line"><span class="comment">#在文件末添加以下内容</span></span><br><span class="line"><span class="comment">#SPARK_HOME </span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/server/spark </span><br><span class="line"><span class="comment">#HADOOP_CONF_DIR </span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop </span><br><span class="line"><span class="comment">#PYSPARK_PYTHON </span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编辑.bashrc文件，添加java和pyspark的Home值</span></span><br><span class="line">$ vim .bashrc</span><br><span class="line"><span class="comment">#添加以下内容</span></span><br><span class="line"><span class="comment">#JAVA_HOME </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241 </span><br><span class="line"><span class="comment">#PYSPARK_PYTHON </span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新加载环境变量文件</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="comment">#进入 /export/server/anaconda3/envs/pyspark/bin/ 文件夹</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/anaconda3/envs/pyspark/bin/</span><br><span class="line"><span class="comment">#开启 Spark</span></span><br><span class="line">$ ./pyspark</span><br></pre></td></tr></table></figure>
<p><strong>看到 pyspark 表示成功</strong><br><img src="/../images/spark.jpg" alt="spark"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试运行基于python的spark解释器环境，在下方运行以下代码</span></span><br><span class="line">$ sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()</span><br><span class="line"><span class="comment">#结果出来后,查看WebUI界面:http://master:4040/</span></span><br></pre></td></tr></table></figure>
<p><img src="/../images/map.jpg" alt="map"></p>
<h2 id="2-Spark-alone-模式"><a href="#2-Spark-alone-模式" class="headerlink" title="2.Spark alone 模式"></a>2.Spark alone 模式</h2><p><strong>tand-alone集群模式中，Spark的各个角色以独立进程的形式存在，并组成Spark集群环境</strong><br>StandAlone集群在进程上主要有三类:</p>
<ul>
<li>主节点Master进程：Master角色，管理整个集群资源，并托管各个任务的Driver</li>
<li>从节点Workers：Worker角色，管理每个机器的资源，分配对应资源来运行Executor（Task）</li>
<li>历史服务器HistoryServer：在Spark Application运行完成以后，保存事件日志数据至HDFS</li>
</ul>
<h3 id="（1）Spark-alone-模式下的新配置"><a href="#（1）Spark-alone-模式下的新配置" class="headerlink" title="（1）Spark alone 模式下的新配置"></a>（1）Spark alone 模式下的新配置</h3><p><strong>前提:三台虚拟机全部安装 Anaconda</strong><br>** 参考Spark（local）模式下的Anaconda的安装文档，在slave1、slave2完成对Anaconda的安装**</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点节点进入 /export/server/spark/conf 修改以下配置文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/conf</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将文件 workers.template 改名为 workers，并配置文件内容</span></span><br><span class="line">$ <span class="built_in">mv</span> workers.template workers</span><br><span class="line"><span class="comment">#修改 workers 文件</span></span><br><span class="line">$ vim workers</span><br><span class="line"><span class="comment">#将 workers 里的 localhost 删除，添加如下内容</span></span><br><span class="line">master </span><br><span class="line">slave1 </span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</span></span><br><span class="line">$ <span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment">#修改 spark-env.sh 文件</span></span><br><span class="line">$ vim spark-env.sh</span><br><span class="line"><span class="comment">#文件最后添加以下内容</span></span><br><span class="line"><span class="comment">## 设置JAVA安装目录 </span></span><br><span class="line">JAVA_HOME=/export/server/jdk </span><br><span class="line"><span class="comment">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 </span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop </span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop </span><br><span class="line"><span class="comment">## 指定spark老大Master的IP和提交任务的通信端口 </span></span><br><span class="line"><span class="comment"># 告知Spark的master运行在哪个机器上 </span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=master </span><br><span class="line"><span class="comment"># 告知sparkmaster的通讯端口 </span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077 </span><br><span class="line"><span class="comment"># 告知spark master的 webui端口 </span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080 </span><br><span class="line"><span class="comment"># worker cpu可用核数 </span></span><br><span class="line">SPARK_WORKER_CORES=1 </span><br><span class="line"><span class="comment"># worker可用内存 </span></span><br><span class="line">SPARK_WORKER_MEMORY=1g </span><br><span class="line"><span class="comment"># worker的工作通讯地址 </span></span><br><span class="line">SPARK_WORKER_PORT=7078 </span><br><span class="line"><span class="comment"># worker的 webui地址 </span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081 </span><br><span class="line"><span class="comment">## 设置历史服务器 </span></span><br><span class="line"><span class="comment"># 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 </span></span><br><span class="line">SPARK_HISTORY_OPTS=<span class="string">&quot;- Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - Dspark.history.fs.cleaner.enabled=true&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#开启hadoop服务</span></span><br><span class="line">$ start-all.sh</span><br><span class="line"><span class="comment">#在HDFS上创建程序运行历史记录存放文件夹</span></span><br><span class="line">$ hadoop fs -<span class="built_in">mkdir</span> /sparklog</span><br><span class="line"><span class="comment">#已存在会显示File exists,属于正常状况</span></span><br><span class="line"><span class="comment">#给sparklog添加权限</span></span><br><span class="line">$ hadoop fs -<span class="built_in">chmod</span> 777 /sparklog</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新进入到 /export/server/spark/conf/ 目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/conf/</span><br><span class="line"><span class="comment">#将spark-defaults.conf.template 文件改为spark-defaults.conf</span></span><br><span class="line">$ <span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="comment">#编辑 spark-defaults.conf</span></span><br><span class="line">$ vim spark-defaults.conf</span><br><span class="line"><span class="comment">#文件最后添加以下内容</span></span><br><span class="line"><span class="comment"># 开启spark的日期记录功能 </span></span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span> </span><br><span class="line"><span class="comment"># 设置spark日志记录的路径 </span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span> hdfs://master:8020/sparklog/ </span><br><span class="line"><span class="comment"># 设置spark日志是否启动压缩 </span></span><br><span class="line">spark.eventLog.compress <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将log4j.properties.template文件改为log4j.properties</span></span><br><span class="line">$ <span class="built_in">mv</span> log4j.properties.template log4j.properties</span><br></pre></td></tr></table></figure>
<p><strong>配置 log4j.properties 文件</strong><br><strong>将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为log4j.rootCategory&#x3D;WARN, console</strong><br><img src="/../images/properties.jpg" alt="properties"></p>
<h3 id="（2）分发"><a href="#（2）分发" class="headerlink" title="（2）分发"></a>（2）分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server</span><br><span class="line">$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave1:<span class="variable">$PWD</span></span><br><span class="line">$ scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave2:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>
<p><strong>在slave1 和 slave2 上做软连接</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave1 节点上</span></span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br><span class="line"><span class="comment">#重新加载环境变量</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 slave2 节点上</span></span><br><span class="line">$ <span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br><span class="line"><span class="comment">#重新加载环境变量</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<p><strong>完成上述操作后，启动历史服务器</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 master 主机上进入 /export/server/spark/sbin 文件目录下</span></span><br><span class="line">$ <span class="built_in">cd</span> /export/server/spark/sbin</span><br><span class="line"><span class="comment">#启动历史服务器</span></span><br><span class="line">$ ./start-history-server.sh</span><br><span class="line"><span class="comment">#访问 WebUI 界面:http://master:18080/</span></span><br></pre></td></tr></table></figure>
<p><img src="/../images/history.jpg" alt="history"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动Spark的Master和Worker进程</span></span><br><span class="line">$ sh start-all.sh</span><br><span class="line"><span class="comment">#通过jps查看进程是否开启master和worker进程</span></span><br><span class="line">$ jps </span><br><span class="line">在查看到 Master 和 Worker 进程后表明成功启动</span><br><span class="line"><span class="comment">#访问 WebUI界面:http://master:8080/</span></span><br></pre></td></tr></table></figure>
<p><img src="/../images/worker.jpg" alt="worker"></p>
<h2 id="以上-就是-Spark-local-amp-stand-alone配置-接下来会带来-Spark-HA-amp-Yarn配置"><a href="#以上-就是-Spark-local-amp-stand-alone配置-接下来会带来-Spark-HA-amp-Yarn配置" class="headerlink" title="以上,就是 Spark local&amp; stand-alone配置,接下来会带来 Spark HA &amp; Yarn配置"></a>以上,就是 Spark local&amp; stand-alone配置,接下来会带来 Spark HA &amp; Yarn配置</h2>
            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
    
        <a href="/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">下一篇</strong>
            <div class="article-nav-title">Spark基础环境配置</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            John Doe &copy; 2022 
            <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a target="_blank" rel="noopener" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>